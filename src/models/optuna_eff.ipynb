{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val', 'train', 'labels', 'test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-04-16 16:38:21,421] A new study created in memory with name: no-name-e0984e9d-64a6-4291-85ea-8c618a719c1a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Validation loss decreased (inf --> 0.691694).  Saving model ...\n",
      "Epoch [1/696], Train Loss: 0.6027, Valid Loss: 0.6917\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# %%\n",
    "# Load the train , test and validation data and labels\n",
    "print(os.listdir(\"../../data/raw/Food\"))\n",
    "labels_df = pd.read_csv(\"../../data/raw/Food/labels/labels.csv\")\n",
    "# Define the data transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Custom dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, dataframe, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        # Load image files\n",
    "        self.image_files = sorted(\n",
    "            [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n",
    "        )\n",
    "        # Initialize a dictionary to map frame identifiers to labels\n",
    "        self.labels_map = {}\n",
    "        # Populate the labels_map\n",
    "        for _, row in dataframe.iterrows():\n",
    "            self.labels_map[row[\"Frame_Number\"]] = row[\"Label\"]\n",
    "        # Filter out image files without a corresponding label\n",
    "        self.image_files = [\n",
    "            img\n",
    "            for img in self.image_files\n",
    "            if os.path.splitext(img)[0] in self.labels_map\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        full_img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(full_img_path).convert(\"RGB\")\n",
    "\n",
    "        frame_identifier = os.path.splitext(img_name)[0]\n",
    "        label = self.labels_map.get(frame_identifier)\n",
    "\n",
    "        # Handle the unlikely case where a label is not found\n",
    "        if label is None:\n",
    "            print(f\"Warning: Label not found for image: {img_name}. Skipping...\")\n",
    "            return None  # This should be handled by your dataloader or skipped\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# %%\n",
    "# Update your DataLoader to skip None types (which we use for missing labels)\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    return default_collate(batch)\n",
    "\n",
    "\n",
    "train_data_path = \"../../data/raw/Food/train\"\n",
    "test_data_path = \"../../data/raw/Food/test\"\n",
    "val_data_path = \"../../data/raw/Food/val\"\n",
    "\n",
    "train_dataset = CustomImageDataset(train_data_path, labels_df, transform)\n",
    "test_dataset = CustomImageDataset(\n",
    "    test_data_path, labels_df, transform\n",
    ")  # Adjust these according to actual splits\n",
    "val_dataset = CustomImageDataset(\n",
    "    val_data_path, labels_df, transform\n",
    ")  # Adjust these according to actual splits\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# %%\n",
    "# !pip install efficientnet_pytorch\n",
    "# !pip install optuna\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet  # Corrected import for your requirement\n",
    "import optuna\n",
    "\n",
    "# EarlyStopping class definition\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                           Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Check for GPU availability and use it if possible\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "def train_model(trial, train_loader, val_loader, num_epochs=5):\n",
    "    # Sample hyperparameters from the trial\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-7, 0.1)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adamax\", \"Adagrad\", \"Adadelta\"])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 500, 1500)\n",
    "\n",
    "    # Initialize and move the model to the specified device\n",
    "    model = EfficientNet.from_name(\"efficientnet-b0\", num_classes=2).to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define optimizer based on the sampled name\n",
    "    optimizer = {\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=learning_rate),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=learning_rate),\n",
    "        \"Adamax\": optim.Adamax(model.parameters(), lr=learning_rate),\n",
    "        \"Adagrad\": optim.Adagrad(model.parameters(), lr=learning_rate),\n",
    "        \"Adadelta\": optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    }[optimizer_name]\n",
    "\n",
    "    # Early stopping initialization\n",
    "    early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        valid_loss /= len(val_loader)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Report intermediate results to Optuna\n",
    "        trial.report(valid_loss, epoch)\n",
    "\n",
    "        # Early stopping call\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        # Print training statistics\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return valid_losses[-1]\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # You can pass additional arguments to `train_model` if needed\n",
    "    return train_model(trial, train_loader, val_loader)\n",
    "\n",
    "# Create a study object and optimize hyperparameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=17)\n",
    "\n",
    "# Get the best hyperparameters and train the final model with them\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import pickle\n",
    "\n",
    "# Assuming 'study' is your Optuna study object\n",
    "with open(\"study.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
