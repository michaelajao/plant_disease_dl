{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val', 'train', 'labels', 'test']\n"
     ]
    }
   ],
   "source": [
    "# Load the train , test and validation data and labels\n",
    "print(os.listdir(\"../../data/raw/Food\"))\n",
    "labels_df = pd.read_csv(\"../../data/raw/Food/labels/labels.csv\")\n",
    "# Define the data transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, dataframe, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        # Load image files\n",
    "        self.image_files = sorted(\n",
    "            [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n",
    "        )\n",
    "        # Initialize a dictionary to map frame identifiers to labels\n",
    "        self.labels_map = {}\n",
    "        # Populate the labels_map\n",
    "        for _, row in dataframe.iterrows():\n",
    "            self.labels_map[row[\"Frame_Number\"]] = row[\"Label\"]\n",
    "        # Filter out image files without a corresponding label\n",
    "        self.image_files = [\n",
    "            img\n",
    "            for img in self.image_files\n",
    "            if os.path.splitext(img)[0] in self.labels_map\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        full_img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(full_img_path).convert(\"RGB\")\n",
    "\n",
    "        frame_identifier = os.path.splitext(img_name)[0]\n",
    "        label = self.labels_map.get(frame_identifier)\n",
    "\n",
    "        # Handle the unlikely case where a label is not found\n",
    "        if label is None:\n",
    "            print(f\"Warning: Label not found for image: {img_name}. Skipping...\")\n",
    "            return None  # This should be handled by your dataloader or skipped\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your DataLoader to skip None types (which we use for missing labels)\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    return default_collate(batch)\n",
    "\n",
    "\n",
    "train_data_path = \"../../data/raw/Food/train\"\n",
    "test_data_path = \"../../data/raw/Food/test\"\n",
    "val_data_path = \"../../data/raw/Food/val\"\n",
    "\n",
    "train_dataset = CustomImageDataset(train_data_path, labels_df, transform)\n",
    "test_dataset = CustomImageDataset(\n",
    "    test_data_path, labels_df, transform\n",
    ")  # Adjust these according to actual splits\n",
    "val_dataset = CustomImageDataset(\n",
    "    val_data_path, labels_df, transform\n",
    ")  # Adjust these according to actual splits\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-04-16 15:39:58,449] A new study created in memory with name: no-name-650f6c80-39d5-4495-95ab-48c1af59bb9d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/882], Train Loss: 0.6320, Valid Loss: 0.8281\n",
      "Epoch [2/882], Train Loss: 0.3778, Valid Loss: 0.8713\n",
      "Epoch [3/882], Train Loss: 0.2597, Valid Loss: 1.0981\n",
      "Epoch [4/882], Train Loss: 0.1452, Valid Loss: 1.3289\n",
      "Epoch [5/882], Train Loss: 0.0957, Valid Loss: 2.0660\n",
      "Epoch [6/882], Train Loss: 0.0595, Valid Loss: 2.6005\n",
      "Epoch [7/882], Train Loss: 0.0925, Valid Loss: 1.7431\n",
      "Epoch [8/882], Train Loss: 0.0754, Valid Loss: 2.5662\n",
      "Epoch [9/882], Train Loss: 0.0370, Valid Loss: 1.9615\n",
      "Epoch [10/882], Train Loss: 0.0432, Valid Loss: 1.8273\n",
      "Epoch [11/882], Train Loss: 0.0359, Valid Loss: 1.0574\n",
      "Epoch [12/882], Train Loss: 0.0476, Valid Loss: 0.5115\n",
      "Epoch [13/882], Train Loss: 0.0442, Valid Loss: 0.2824\n",
      "Epoch [14/882], Train Loss: 0.0219, Valid Loss: 0.1710\n",
      "Epoch [15/882], Train Loss: 0.0328, Valid Loss: 0.2698\n",
      "Epoch [16/882], Train Loss: 0.0152, Valid Loss: 0.2669\n",
      "Epoch [17/882], Train Loss: 0.0135, Valid Loss: 0.1844\n",
      "Epoch [18/882], Train Loss: 0.0332, Valid Loss: 0.1678\n",
      "Epoch [19/882], Train Loss: 0.0119, Valid Loss: 0.2374\n",
      "Epoch [20/882], Train Loss: 0.0052, Valid Loss: 0.1492\n",
      "Epoch [21/882], Train Loss: 0.0076, Valid Loss: 0.3879\n",
      "Epoch [22/882], Train Loss: 0.0139, Valid Loss: 0.1910\n",
      "Epoch [23/882], Train Loss: 0.0287, Valid Loss: 0.3597\n",
      "Epoch [24/882], Train Loss: 0.0385, Valid Loss: 0.2383\n",
      "Epoch [25/882], Train Loss: 0.0100, Valid Loss: 0.2265\n",
      "Epoch [26/882], Train Loss: 0.0048, Valid Loss: 0.2004\n",
      "Epoch [27/882], Train Loss: 0.0096, Valid Loss: 0.2603\n",
      "Epoch [28/882], Train Loss: 0.0132, Valid Loss: 0.3904\n",
      "Epoch [29/882], Train Loss: 0.0117, Valid Loss: 0.1667\n",
      "Epoch [30/882], Train Loss: 0.0063, Valid Loss: 0.1657\n",
      "Epoch [31/882], Train Loss: 0.0086, Valid Loss: 0.2658\n",
      "Epoch [32/882], Train Loss: 0.0016, Valid Loss: 0.2115\n",
      "Epoch [33/882], Train Loss: 0.0023, Valid Loss: 0.2796\n",
      "Epoch [34/882], Train Loss: 0.0074, Valid Loss: 0.2435\n",
      "Epoch [35/882], Train Loss: 0.0045, Valid Loss: 0.6216\n",
      "Epoch [36/882], Train Loss: 0.0226, Valid Loss: 0.3176\n",
      "Epoch [37/882], Train Loss: 0.0137, Valid Loss: 0.1531\n",
      "Epoch [38/882], Train Loss: 0.0072, Valid Loss: 0.3719\n",
      "Epoch [39/882], Train Loss: 0.0081, Valid Loss: 0.5171\n",
      "Epoch [40/882], Train Loss: 0.0045, Valid Loss: 0.1981\n",
      "Epoch [41/882], Train Loss: 0.0038, Valid Loss: 0.1684\n",
      "Epoch [42/882], Train Loss: 0.0017, Valid Loss: 0.1735\n",
      "Epoch [43/882], Train Loss: 0.0030, Valid Loss: 0.2477\n",
      "Epoch [44/882], Train Loss: 0.0017, Valid Loss: 0.1653\n",
      "Epoch [45/882], Train Loss: 0.0007, Valid Loss: 0.2515\n",
      "Epoch [46/882], Train Loss: 0.0152, Valid Loss: 0.3202\n",
      "Epoch [47/882], Train Loss: 0.0181, Valid Loss: 0.3714\n",
      "Epoch [48/882], Train Loss: 0.0160, Valid Loss: 0.4213\n",
      "Epoch [49/882], Train Loss: 0.0050, Valid Loss: 0.1361\n",
      "Epoch [50/882], Train Loss: 0.0087, Valid Loss: 0.1175\n",
      "Epoch [51/882], Train Loss: 0.0014, Valid Loss: 0.1316\n",
      "Epoch [52/882], Train Loss: 0.0014, Valid Loss: 0.1429\n",
      "Epoch [53/882], Train Loss: 0.0008, Valid Loss: 0.2291\n",
      "Epoch [54/882], Train Loss: 0.0014, Valid Loss: 0.2508\n",
      "Epoch [55/882], Train Loss: 0.0159, Valid Loss: 0.3209\n",
      "Epoch [56/882], Train Loss: 0.0261, Valid Loss: 0.2221\n",
      "Epoch [57/882], Train Loss: 0.0080, Valid Loss: 0.1454\n",
      "Epoch [58/882], Train Loss: 0.0075, Valid Loss: 0.1009\n",
      "Epoch [59/882], Train Loss: 0.0009, Valid Loss: 0.2230\n",
      "Epoch [60/882], Train Loss: 0.0100, Valid Loss: 0.1085\n",
      "Epoch [61/882], Train Loss: 0.0022, Valid Loss: 0.1014\n",
      "Epoch [62/882], Train Loss: 0.0050, Valid Loss: 0.1260\n",
      "Epoch [63/882], Train Loss: 0.0019, Valid Loss: 0.1523\n",
      "Epoch [64/882], Train Loss: 0.0003, Valid Loss: 0.1139\n",
      "Epoch [65/882], Train Loss: 0.0005, Valid Loss: 0.0887\n",
      "Epoch [66/882], Train Loss: 0.0003, Valid Loss: 0.1409\n",
      "Epoch [67/882], Train Loss: 0.0075, Valid Loss: 0.1370\n",
      "Epoch [68/882], Train Loss: 0.0227, Valid Loss: 0.1409\n",
      "Epoch [69/882], Train Loss: 0.0031, Valid Loss: 0.1475\n",
      "Epoch [70/882], Train Loss: 0.0096, Valid Loss: 0.0847\n",
      "Epoch [71/882], Train Loss: 0.0064, Valid Loss: 0.1966\n",
      "Epoch [72/882], Train Loss: 0.0018, Valid Loss: 0.1177\n",
      "Epoch [73/882], Train Loss: 0.0008, Valid Loss: 0.1078\n",
      "Epoch [74/882], Train Loss: 0.0020, Valid Loss: 0.1548\n",
      "Epoch [75/882], Train Loss: 0.0039, Valid Loss: 0.1624\n",
      "Epoch [76/882], Train Loss: 0.0172, Valid Loss: 0.3689\n",
      "Epoch [77/882], Train Loss: 0.0128, Valid Loss: 0.3084\n",
      "Epoch [78/882], Train Loss: 0.0131, Valid Loss: 0.3457\n",
      "Epoch [79/882], Train Loss: 0.0012, Valid Loss: 0.1697\n",
      "Epoch [80/882], Train Loss: 0.0009, Valid Loss: 0.1805\n",
      "Epoch [81/882], Train Loss: 0.0004, Valid Loss: 0.1575\n",
      "Epoch [82/882], Train Loss: 0.0009, Valid Loss: 0.1905\n",
      "Epoch [83/882], Train Loss: 0.0015, Valid Loss: 0.2289\n",
      "Epoch [84/882], Train Loss: 0.0012, Valid Loss: 0.1474\n",
      "Epoch [85/882], Train Loss: 0.0130, Valid Loss: 0.1774\n",
      "Epoch [86/882], Train Loss: 0.0012, Valid Loss: 0.2635\n",
      "Epoch [87/882], Train Loss: 0.0004, Valid Loss: 0.2474\n",
      "Epoch [88/882], Train Loss: 0.0067, Valid Loss: 0.2675\n",
      "Epoch [89/882], Train Loss: 0.0081, Valid Loss: 0.1621\n",
      "Epoch [90/882], Train Loss: 0.0006, Valid Loss: 0.3023\n",
      "Epoch [91/882], Train Loss: 0.0002, Valid Loss: 0.2908\n",
      "Epoch [92/882], Train Loss: 0.0008, Valid Loss: 0.2187\n",
      "Epoch [93/882], Train Loss: 0.0012, Valid Loss: 0.2626\n",
      "Epoch [94/882], Train Loss: 0.0001, Valid Loss: 0.2517\n",
      "Epoch [95/882], Train Loss: 0.0002, Valid Loss: 0.2213\n",
      "Epoch [96/882], Train Loss: 0.0002, Valid Loss: 0.2513\n",
      "Epoch [97/882], Train Loss: 0.0002, Valid Loss: 0.2291\n",
      "Epoch [98/882], Train Loss: 0.0001, Valid Loss: 0.2242\n",
      "Epoch [99/882], Train Loss: 0.0001, Valid Loss: 0.2385\n",
      "Epoch [100/882], Train Loss: 0.0031, Valid Loss: 0.5288\n",
      "Epoch [101/882], Train Loss: 0.0107, Valid Loss: 0.2084\n",
      "Epoch [102/882], Train Loss: 0.0086, Valid Loss: 0.2622\n",
      "Epoch [103/882], Train Loss: 0.0151, Valid Loss: 0.5681\n",
      "Epoch [104/882], Train Loss: 0.0048, Valid Loss: 0.4389\n",
      "Epoch [105/882], Train Loss: 0.0061, Valid Loss: 0.1752\n",
      "Epoch [106/882], Train Loss: 0.0005, Valid Loss: 0.1614\n",
      "Epoch [107/882], Train Loss: 0.0002, Valid Loss: 0.1929\n",
      "Epoch [108/882], Train Loss: 0.0001, Valid Loss: 0.2167\n",
      "Epoch [109/882], Train Loss: 0.0021, Valid Loss: 0.2514\n",
      "Epoch [110/882], Train Loss: 0.0002, Valid Loss: 0.1919\n",
      "Epoch [111/882], Train Loss: 0.0003, Valid Loss: 0.1723\n",
      "Epoch [112/882], Train Loss: 0.0002, Valid Loss: 0.1887\n",
      "Epoch [113/882], Train Loss: 0.0001, Valid Loss: 0.2063\n",
      "Epoch [114/882], Train Loss: 0.0000, Valid Loss: 0.2108\n",
      "Epoch [115/882], Train Loss: 0.0000, Valid Loss: 0.2090\n",
      "Epoch [116/882], Train Loss: 0.0001, Valid Loss: 0.2239\n",
      "Epoch [117/882], Train Loss: 0.0001, Valid Loss: 0.2021\n",
      "Epoch [118/882], Train Loss: 0.0001, Valid Loss: 0.1927\n",
      "Epoch [119/882], Train Loss: 0.0001, Valid Loss: 0.1966\n",
      "Epoch [120/882], Train Loss: 0.0000, Valid Loss: 0.1927\n",
      "Epoch [121/882], Train Loss: 0.0000, Valid Loss: 0.1898\n",
      "Epoch [122/882], Train Loss: 0.0000, Valid Loss: 0.1878\n",
      "Epoch [123/882], Train Loss: 0.0001, Valid Loss: 0.1984\n",
      "Epoch [124/882], Train Loss: 0.0000, Valid Loss: 0.2245\n",
      "Epoch [125/882], Train Loss: 0.0002, Valid Loss: 0.2210\n",
      "Epoch [126/882], Train Loss: 0.0005, Valid Loss: 0.4506\n",
      "Epoch [127/882], Train Loss: 0.0070, Valid Loss: 0.4556\n",
      "Epoch [128/882], Train Loss: 0.0394, Valid Loss: 0.3579\n",
      "Epoch [129/882], Train Loss: 0.0088, Valid Loss: 0.3538\n",
      "Epoch [130/882], Train Loss: 0.0098, Valid Loss: 0.1614\n",
      "Epoch [131/882], Train Loss: 0.0007, Valid Loss: 0.1765\n",
      "Epoch [132/882], Train Loss: 0.0004, Valid Loss: 0.1781\n",
      "Epoch [133/882], Train Loss: 0.0003, Valid Loss: 0.1918\n",
      "Epoch [134/882], Train Loss: 0.0002, Valid Loss: 0.2279\n",
      "Epoch [135/882], Train Loss: 0.0026, Valid Loss: 0.2898\n",
      "Epoch [136/882], Train Loss: 0.0007, Valid Loss: 0.1904\n",
      "Epoch [137/882], Train Loss: 0.0069, Valid Loss: 0.2231\n",
      "Epoch [138/882], Train Loss: 0.0035, Valid Loss: 0.4362\n",
      "Epoch [139/882], Train Loss: 0.0009, Valid Loss: 0.1582\n",
      "Epoch [140/882], Train Loss: 0.0009, Valid Loss: 0.1935\n",
      "Epoch [141/882], Train Loss: 0.0079, Valid Loss: 0.1357\n",
      "Epoch [142/882], Train Loss: 0.0006, Valid Loss: 0.1762\n",
      "Epoch [143/882], Train Loss: 0.0002, Valid Loss: 0.1936\n",
      "Epoch [144/882], Train Loss: 0.0001, Valid Loss: 0.1937\n",
      "Epoch [145/882], Train Loss: 0.0001, Valid Loss: 0.2019\n",
      "Epoch [146/882], Train Loss: 0.0000, Valid Loss: 0.1939\n",
      "Epoch [147/882], Train Loss: 0.0001, Valid Loss: 0.1978\n",
      "Epoch [148/882], Train Loss: 0.0006, Valid Loss: 0.1861\n",
      "Epoch [149/882], Train Loss: 0.0008, Valid Loss: 0.3021\n",
      "Epoch [150/882], Train Loss: 0.0014, Valid Loss: 0.2274\n",
      "Epoch [151/882], Train Loss: 0.0010, Valid Loss: 0.2193\n",
      "Epoch [152/882], Train Loss: 0.0005, Valid Loss: 0.2471\n",
      "Epoch [153/882], Train Loss: 0.0000, Valid Loss: 0.1815\n",
      "Epoch [154/882], Train Loss: 0.0000, Valid Loss: 0.1841\n",
      "Epoch [155/882], Train Loss: 0.0000, Valid Loss: 0.1904\n",
      "Epoch [156/882], Train Loss: 0.0001, Valid Loss: 0.2380\n",
      "Epoch [157/882], Train Loss: 0.0000, Valid Loss: 0.2575\n",
      "Epoch [158/882], Train Loss: 0.0000, Valid Loss: 0.2531\n",
      "Epoch [159/882], Train Loss: 0.0001, Valid Loss: 0.2660\n",
      "Epoch [160/882], Train Loss: 0.0000, Valid Loss: 0.2585\n",
      "Epoch [161/882], Train Loss: 0.0000, Valid Loss: 0.2577\n",
      "Epoch [162/882], Train Loss: 0.0004, Valid Loss: 0.3458\n",
      "Epoch [163/882], Train Loss: 0.0002, Valid Loss: 0.7773\n",
      "Epoch [164/882], Train Loss: 0.0015, Valid Loss: 0.4260\n",
      "Epoch [165/882], Train Loss: 0.0208, Valid Loss: 0.4459\n",
      "Epoch [166/882], Train Loss: 0.0067, Valid Loss: 0.2392\n",
      "Epoch [167/882], Train Loss: 0.0018, Valid Loss: 0.1797\n",
      "Epoch [168/882], Train Loss: 0.0004, Valid Loss: 0.1719\n",
      "Epoch [169/882], Train Loss: 0.0003, Valid Loss: 0.1556\n",
      "Epoch [170/882], Train Loss: 0.0001, Valid Loss: 0.1623\n",
      "Epoch [171/882], Train Loss: 0.0001, Valid Loss: 0.1684\n",
      "Epoch [172/882], Train Loss: 0.0001, Valid Loss: 0.1776\n",
      "Epoch [173/882], Train Loss: 0.0000, Valid Loss: 0.1955\n",
      "Epoch [174/882], Train Loss: 0.0000, Valid Loss: 0.2024\n",
      "Epoch [175/882], Train Loss: 0.0000, Valid Loss: 0.2058\n",
      "Epoch [176/882], Train Loss: 0.0001, Valid Loss: 0.1930\n",
      "Epoch [177/882], Train Loss: 0.0002, Valid Loss: 0.1729\n",
      "Epoch [178/882], Train Loss: 0.0002, Valid Loss: 0.2012\n",
      "Epoch [179/882], Train Loss: 0.0000, Valid Loss: 0.2071\n",
      "Epoch [180/882], Train Loss: 0.0000, Valid Loss: 0.2158\n",
      "Epoch [181/882], Train Loss: 0.0001, Valid Loss: 0.2201\n",
      "Epoch [182/882], Train Loss: 0.0011, Valid Loss: 0.2620\n",
      "Epoch [183/882], Train Loss: 0.0005, Valid Loss: 0.2544\n",
      "Epoch [184/882], Train Loss: 0.0076, Valid Loss: 0.4764\n",
      "Epoch [185/882], Train Loss: 0.0213, Valid Loss: 0.3889\n",
      "Epoch [186/882], Train Loss: 0.0064, Valid Loss: 0.3830\n",
      "Epoch [187/882], Train Loss: 0.0104, Valid Loss: 0.1811\n",
      "Epoch [188/882], Train Loss: 0.0009, Valid Loss: 0.3115\n",
      "Epoch [189/882], Train Loss: 0.0004, Valid Loss: 0.3149\n",
      "Epoch [190/882], Train Loss: 0.0010, Valid Loss: 0.3049\n",
      "Epoch [191/882], Train Loss: 0.0036, Valid Loss: 0.2146\n",
      "Epoch [192/882], Train Loss: 0.0093, Valid Loss: 0.5465\n",
      "Epoch [193/882], Train Loss: 0.0072, Valid Loss: 0.3685\n",
      "Epoch [194/882], Train Loss: 0.0007, Valid Loss: 0.2546\n",
      "Epoch [195/882], Train Loss: 0.0003, Valid Loss: 0.2417\n",
      "Epoch [196/882], Train Loss: 0.0001, Valid Loss: 0.2368\n",
      "Epoch [197/882], Train Loss: 0.0003, Valid Loss: 0.2516\n",
      "Epoch [198/882], Train Loss: 0.0012, Valid Loss: 0.2324\n",
      "Epoch [199/882], Train Loss: 0.0011, Valid Loss: 0.2840\n",
      "Epoch [200/882], Train Loss: 0.0001, Valid Loss: 0.2419\n",
      "Epoch [201/882], Train Loss: 0.0000, Valid Loss: 0.2425\n",
      "Epoch [202/882], Train Loss: 0.0000, Valid Loss: 0.2483\n",
      "Epoch [203/882], Train Loss: 0.0019, Valid Loss: 0.2285\n",
      "Epoch [204/882], Train Loss: 0.0091, Valid Loss: 0.2988\n",
      "Epoch [205/882], Train Loss: 0.0094, Valid Loss: 0.3317\n",
      "Epoch [206/882], Train Loss: 0.0017, Valid Loss: 0.1835\n",
      "Epoch [207/882], Train Loss: 0.0007, Valid Loss: 0.1796\n",
      "Epoch [208/882], Train Loss: 0.0002, Valid Loss: 0.1886\n",
      "Epoch [209/882], Train Loss: 0.0002, Valid Loss: 0.1900\n",
      "Epoch [210/882], Train Loss: 0.0002, Valid Loss: 0.2075\n",
      "Epoch [211/882], Train Loss: 0.0000, Valid Loss: 0.2218\n",
      "Epoch [212/882], Train Loss: 0.0004, Valid Loss: 0.2384\n",
      "Epoch [213/882], Train Loss: 0.0000, Valid Loss: 0.2270\n",
      "Epoch [214/882], Train Loss: 0.0008, Valid Loss: 0.2254\n",
      "Epoch [215/882], Train Loss: 0.0009, Valid Loss: 0.2466\n",
      "Epoch [216/882], Train Loss: 0.0010, Valid Loss: 0.3390\n",
      "Epoch [217/882], Train Loss: 0.0052, Valid Loss: 0.2209\n",
      "Epoch [218/882], Train Loss: 0.0040, Valid Loss: 0.3414\n",
      "Epoch [219/882], Train Loss: 0.0007, Valid Loss: 0.3149\n",
      "Epoch [220/882], Train Loss: 0.0001, Valid Loss: 0.3047\n",
      "Epoch [221/882], Train Loss: 0.0036, Valid Loss: 0.2800\n",
      "Epoch [222/882], Train Loss: 0.0128, Valid Loss: 0.4302\n",
      "Epoch [223/882], Train Loss: 0.0039, Valid Loss: 0.3461\n",
      "Epoch [224/882], Train Loss: 0.0034, Valid Loss: 0.2466\n",
      "Epoch [225/882], Train Loss: 0.0036, Valid Loss: 0.1961\n",
      "Epoch [226/882], Train Loss: 0.0020, Valid Loss: 0.1473\n",
      "Epoch [227/882], Train Loss: 0.0009, Valid Loss: 0.1828\n",
      "Epoch [228/882], Train Loss: 0.0031, Valid Loss: 0.1186\n",
      "Epoch [229/882], Train Loss: 0.0002, Valid Loss: 0.1113\n",
      "Epoch [230/882], Train Loss: 0.0001, Valid Loss: 0.1192\n",
      "Epoch [231/882], Train Loss: 0.0000, Valid Loss: 0.1280\n",
      "Epoch [232/882], Train Loss: 0.0001, Valid Loss: 0.1418\n",
      "Epoch [233/882], Train Loss: 0.0000, Valid Loss: 0.1550\n",
      "Epoch [234/882], Train Loss: 0.0076, Valid Loss: 0.1744\n",
      "Epoch [235/882], Train Loss: 0.0035, Valid Loss: 0.3199\n",
      "Epoch [236/882], Train Loss: 0.0081, Valid Loss: 0.3580\n",
      "Epoch [237/882], Train Loss: 0.0010, Valid Loss: 0.3303\n",
      "Epoch [238/882], Train Loss: 0.0002, Valid Loss: 0.2787\n",
      "Epoch [239/882], Train Loss: 0.0001, Valid Loss: 0.2416\n",
      "Epoch [240/882], Train Loss: 0.0001, Valid Loss: 0.2286\n",
      "Epoch [241/882], Train Loss: 0.0000, Valid Loss: 0.2229\n",
      "Epoch [242/882], Train Loss: 0.0005, Valid Loss: 0.2118\n",
      "Epoch [243/882], Train Loss: 0.0008, Valid Loss: 0.1635\n",
      "Epoch [244/882], Train Loss: 0.0001, Valid Loss: 0.1732\n",
      "Epoch [245/882], Train Loss: 0.0001, Valid Loss: 0.1903\n",
      "Epoch [246/882], Train Loss: 0.0000, Valid Loss: 0.2058\n",
      "Epoch [247/882], Train Loss: 0.0000, Valid Loss: 0.2173\n",
      "Epoch [248/882], Train Loss: 0.0009, Valid Loss: 0.1617\n",
      "Epoch [249/882], Train Loss: 0.0008, Valid Loss: 0.1976\n",
      "Epoch [250/882], Train Loss: 0.0000, Valid Loss: 0.2212\n",
      "Epoch [251/882], Train Loss: 0.0000, Valid Loss: 0.2392\n",
      "Epoch [252/882], Train Loss: 0.0000, Valid Loss: 0.2424\n",
      "Epoch [253/882], Train Loss: 0.0004, Valid Loss: 0.3047\n",
      "Epoch [254/882], Train Loss: 0.0001, Valid Loss: 0.2225\n",
      "Epoch [255/882], Train Loss: 0.0000, Valid Loss: 0.2115\n",
      "Epoch [256/882], Train Loss: 0.0000, Valid Loss: 0.2111\n",
      "Epoch [257/882], Train Loss: 0.0000, Valid Loss: 0.2120\n",
      "Epoch [258/882], Train Loss: 0.0000, Valid Loss: 0.2119\n",
      "Epoch [259/882], Train Loss: 0.0000, Valid Loss: 0.2136\n",
      "Epoch [260/882], Train Loss: 0.0000, Valid Loss: 0.2147\n",
      "Epoch [261/882], Train Loss: 0.0000, Valid Loss: 0.2230\n",
      "Epoch [262/882], Train Loss: 0.0001, Valid Loss: 0.2367\n",
      "Epoch [263/882], Train Loss: 0.0001, Valid Loss: 0.3389\n",
      "Epoch [264/882], Train Loss: 0.0000, Valid Loss: 0.3369\n",
      "Epoch [265/882], Train Loss: 0.0001, Valid Loss: 0.3533\n",
      "Epoch [266/882], Train Loss: 0.0009, Valid Loss: 0.2802\n",
      "Epoch [267/882], Train Loss: 0.0050, Valid Loss: 0.4370\n",
      "Epoch [268/882], Train Loss: 0.0084, Valid Loss: 0.6746\n",
      "Epoch [269/882], Train Loss: 0.0124, Valid Loss: 0.4355\n",
      "Epoch [270/882], Train Loss: 0.0015, Valid Loss: 0.2213\n",
      "Epoch [271/882], Train Loss: 0.0002, Valid Loss: 0.2733\n",
      "Epoch [272/882], Train Loss: 0.0001, Valid Loss: 0.2743\n",
      "Epoch [273/882], Train Loss: 0.0000, Valid Loss: 0.2760\n",
      "Epoch [274/882], Train Loss: 0.0000, Valid Loss: 0.2772\n",
      "Epoch [275/882], Train Loss: 0.0001, Valid Loss: 0.2765\n",
      "Epoch [276/882], Train Loss: 0.0001, Valid Loss: 0.2796\n",
      "Epoch [277/882], Train Loss: 0.0001, Valid Loss: 0.2829\n",
      "Epoch [278/882], Train Loss: 0.0001, Valid Loss: 0.2861\n",
      "Epoch [279/882], Train Loss: 0.0000, Valid Loss: 0.2852\n",
      "Epoch [280/882], Train Loss: 0.0002, Valid Loss: 0.2042\n",
      "Epoch [281/882], Train Loss: 0.0000, Valid Loss: 0.2036\n",
      "Epoch [282/882], Train Loss: 0.0001, Valid Loss: 0.2239\n",
      "Epoch [283/882], Train Loss: 0.0000, Valid Loss: 0.2315\n",
      "Epoch [284/882], Train Loss: 0.0000, Valid Loss: 0.2480\n",
      "Epoch [285/882], Train Loss: 0.0000, Valid Loss: 0.2536\n",
      "Epoch [286/882], Train Loss: 0.0000, Valid Loss: 0.2547\n",
      "Epoch [287/882], Train Loss: 0.0001, Valid Loss: 0.2543\n",
      "Epoch [288/882], Train Loss: 0.0000, Valid Loss: 0.2674\n",
      "Epoch [289/882], Train Loss: 0.0000, Valid Loss: 0.2726\n",
      "Epoch [290/882], Train Loss: 0.0000, Valid Loss: 0.2696\n",
      "Epoch [291/882], Train Loss: 0.0000, Valid Loss: 0.2732\n",
      "Epoch [292/882], Train Loss: 0.0043, Valid Loss: 0.2737\n",
      "Epoch [293/882], Train Loss: 0.0026, Valid Loss: 0.3637\n",
      "Epoch [294/882], Train Loss: 0.0064, Valid Loss: 0.6195\n",
      "Epoch [295/882], Train Loss: 0.0008, Valid Loss: 0.3850\n",
      "Epoch [296/882], Train Loss: 0.0003, Valid Loss: 0.3151\n",
      "Epoch [297/882], Train Loss: 0.0009, Valid Loss: 0.3616\n",
      "Epoch [298/882], Train Loss: 0.0001, Valid Loss: 0.3448\n",
      "Epoch [299/882], Train Loss: 0.0001, Valid Loss: 0.3013\n",
      "Epoch [300/882], Train Loss: 0.0000, Valid Loss: 0.2871\n",
      "Epoch [301/882], Train Loss: 0.0003, Valid Loss: 0.2427\n",
      "Epoch [302/882], Train Loss: 0.0002, Valid Loss: 0.2934\n",
      "Epoch [303/882], Train Loss: 0.0019, Valid Loss: 0.2514\n",
      "Epoch [304/882], Train Loss: 0.0015, Valid Loss: 0.1269\n",
      "Epoch [305/882], Train Loss: 0.0001, Valid Loss: 0.1427\n",
      "Epoch [306/882], Train Loss: 0.0030, Valid Loss: 0.3119\n",
      "Epoch [307/882], Train Loss: 0.0138, Valid Loss: 1.0383\n",
      "Epoch [308/882], Train Loss: 0.0008, Valid Loss: 0.7006\n",
      "Epoch [309/882], Train Loss: 0.0004, Valid Loss: 0.5719\n",
      "Epoch [310/882], Train Loss: 0.0002, Valid Loss: 0.5583\n",
      "Epoch [311/882], Train Loss: 0.0001, Valid Loss: 0.5372\n",
      "Epoch [312/882], Train Loss: 0.0001, Valid Loss: 0.5070\n",
      "Epoch [313/882], Train Loss: 0.0001, Valid Loss: 0.5120\n",
      "Epoch [314/882], Train Loss: 0.0000, Valid Loss: 0.4949\n",
      "Epoch [315/882], Train Loss: 0.0000, Valid Loss: 0.4832\n",
      "Epoch [316/882], Train Loss: 0.0000, Valid Loss: 0.4773\n",
      "Epoch [317/882], Train Loss: 0.0000, Valid Loss: 0.4778\n",
      "Epoch [318/882], Train Loss: 0.0000, Valid Loss: 0.4807\n",
      "Epoch [319/882], Train Loss: 0.0002, Valid Loss: 0.4524\n",
      "Epoch [320/882], Train Loss: 0.0002, Valid Loss: 0.4273\n",
      "Epoch [321/882], Train Loss: 0.0000, Valid Loss: 0.4317\n",
      "Epoch [322/882], Train Loss: 0.0000, Valid Loss: 0.4367\n",
      "Epoch [323/882], Train Loss: 0.0000, Valid Loss: 0.4385\n",
      "Epoch [324/882], Train Loss: 0.0000, Valid Loss: 0.4419\n",
      "Epoch [325/882], Train Loss: 0.0000, Valid Loss: 0.4451\n",
      "Epoch [326/882], Train Loss: 0.0000, Valid Loss: 0.4504\n",
      "Epoch [327/882], Train Loss: 0.0000, Valid Loss: 0.4492\n",
      "Epoch [328/882], Train Loss: 0.0000, Valid Loss: 0.4513\n",
      "Epoch [329/882], Train Loss: 0.0000, Valid Loss: 0.4536\n",
      "Epoch [330/882], Train Loss: 0.0000, Valid Loss: 0.4455\n",
      "Epoch [331/882], Train Loss: 0.0000, Valid Loss: 0.4469\n",
      "Epoch [332/882], Train Loss: 0.0000, Valid Loss: 0.4499\n",
      "Epoch [333/882], Train Loss: 0.0005, Valid Loss: 0.4721\n",
      "Epoch [334/882], Train Loss: 0.0018, Valid Loss: 0.3507\n",
      "Epoch [335/882], Train Loss: 0.0052, Valid Loss: 0.3299\n",
      "Epoch [336/882], Train Loss: 0.0004, Valid Loss: 0.3713\n",
      "Epoch [337/882], Train Loss: 0.0021, Valid Loss: 0.2792\n",
      "Epoch [338/882], Train Loss: 0.0001, Valid Loss: 0.2984\n",
      "Epoch [339/882], Train Loss: 0.0001, Valid Loss: 0.3157\n",
      "Epoch [340/882], Train Loss: 0.0000, Valid Loss: 0.3306\n",
      "Epoch [341/882], Train Loss: 0.0005, Valid Loss: 0.3763\n",
      "Epoch [342/882], Train Loss: 0.0014, Valid Loss: 0.6513\n",
      "Epoch [343/882], Train Loss: 0.0000, Valid Loss: 0.5353\n",
      "Epoch [344/882], Train Loss: 0.0000, Valid Loss: 0.5129\n",
      "Epoch [345/882], Train Loss: 0.0000, Valid Loss: 0.5228\n",
      "Epoch [346/882], Train Loss: 0.0000, Valid Loss: 0.4997\n",
      "Epoch [347/882], Train Loss: 0.0008, Valid Loss: 0.4138\n",
      "Epoch [348/882], Train Loss: 0.0060, Valid Loss: 0.8857\n",
      "Epoch [349/882], Train Loss: 0.0037, Valid Loss: 0.4666\n",
      "Epoch [350/882], Train Loss: 0.0004, Valid Loss: 0.3773\n",
      "Epoch [351/882], Train Loss: 0.0002, Valid Loss: 0.3754\n",
      "Epoch [352/882], Train Loss: 0.0012, Valid Loss: 0.2444\n",
      "Epoch [353/882], Train Loss: 0.0086, Valid Loss: 0.2883\n",
      "Epoch [354/882], Train Loss: 0.0014, Valid Loss: 0.2054\n",
      "Epoch [355/882], Train Loss: 0.0001, Valid Loss: 0.2240\n",
      "Epoch [356/882], Train Loss: 0.0000, Valid Loss: 0.2317\n",
      "Epoch [357/882], Train Loss: 0.0001, Valid Loss: 0.2380\n",
      "Epoch [358/882], Train Loss: 0.0001, Valid Loss: 0.2466\n",
      "Epoch [359/882], Train Loss: 0.0001, Valid Loss: 0.2590\n",
      "Epoch [360/882], Train Loss: 0.0000, Valid Loss: 0.2675\n",
      "Epoch [361/882], Train Loss: 0.0000, Valid Loss: 0.2699\n",
      "Epoch [362/882], Train Loss: 0.0000, Valid Loss: 0.2722\n",
      "Epoch [363/882], Train Loss: 0.0000, Valid Loss: 0.2698\n",
      "Epoch [364/882], Train Loss: 0.0000, Valid Loss: 0.2680\n",
      "Epoch [365/882], Train Loss: 0.0000, Valid Loss: 0.2663\n",
      "Epoch [366/882], Train Loss: 0.0000, Valid Loss: 0.2583\n",
      "Epoch [367/882], Train Loss: 0.0000, Valid Loss: 0.2586\n",
      "Epoch [368/882], Train Loss: 0.0000, Valid Loss: 0.2616\n",
      "Epoch [369/882], Train Loss: 0.0000, Valid Loss: 0.2596\n",
      "Epoch [370/882], Train Loss: 0.0010, Valid Loss: 0.2750\n",
      "Epoch [371/882], Train Loss: 0.0047, Valid Loss: 0.2598\n",
      "Epoch [372/882], Train Loss: 0.0036, Valid Loss: 0.3169\n",
      "Epoch [373/882], Train Loss: 0.0057, Valid Loss: 0.4855\n",
      "Epoch [374/882], Train Loss: 0.0050, Valid Loss: 0.3150\n",
      "Epoch [375/882], Train Loss: 0.0030, Valid Loss: 0.2881\n",
      "Epoch [376/882], Train Loss: 0.0003, Valid Loss: 0.3074\n",
      "Epoch [377/882], Train Loss: 0.0003, Valid Loss: 0.3704\n",
      "Epoch [378/882], Train Loss: 0.0001, Valid Loss: 0.3690\n",
      "Epoch [379/882], Train Loss: 0.0001, Valid Loss: 0.3609\n",
      "Epoch [380/882], Train Loss: 0.0010, Valid Loss: 0.3783\n",
      "Epoch [381/882], Train Loss: 0.0002, Valid Loss: 0.3863\n",
      "Epoch [382/882], Train Loss: 0.0005, Valid Loss: 0.3877\n",
      "Epoch [383/882], Train Loss: 0.0000, Valid Loss: 0.3902\n",
      "Epoch [384/882], Train Loss: 0.0003, Valid Loss: 0.4339\n",
      "Epoch [385/882], Train Loss: 0.0004, Valid Loss: 0.3712\n",
      "Epoch [386/882], Train Loss: 0.0085, Valid Loss: 0.5091\n",
      "Epoch [387/882], Train Loss: 0.0026, Valid Loss: 0.4737\n",
      "Epoch [388/882], Train Loss: 0.0003, Valid Loss: 0.4372\n",
      "Epoch [389/882], Train Loss: 0.0017, Valid Loss: 0.5621\n",
      "Epoch [390/882], Train Loss: 0.0003, Valid Loss: 0.4930\n",
      "Epoch [391/882], Train Loss: 0.0001, Valid Loss: 0.4833\n",
      "Epoch [392/882], Train Loss: 0.0000, Valid Loss: 0.4566\n",
      "Epoch [393/882], Train Loss: 0.0000, Valid Loss: 0.4402\n",
      "Epoch [394/882], Train Loss: 0.0000, Valid Loss: 0.4338\n",
      "Epoch [395/882], Train Loss: 0.0000, Valid Loss: 0.4333\n",
      "Epoch [396/882], Train Loss: 0.0000, Valid Loss: 0.4309\n",
      "Epoch [397/882], Train Loss: 0.0025, Valid Loss: 0.5956\n",
      "Epoch [398/882], Train Loss: 0.0041, Valid Loss: 0.4237\n",
      "Epoch [399/882], Train Loss: 0.0088, Valid Loss: 0.3112\n",
      "Epoch [400/882], Train Loss: 0.0009, Valid Loss: 0.1826\n",
      "Epoch [401/882], Train Loss: 0.0007, Valid Loss: 0.1927\n",
      "Epoch [402/882], Train Loss: 0.0004, Valid Loss: 0.2415\n",
      "Epoch [403/882], Train Loss: 0.0000, Valid Loss: 0.2531\n",
      "Epoch [404/882], Train Loss: 0.0001, Valid Loss: 0.2759\n",
      "Epoch [405/882], Train Loss: 0.0000, Valid Loss: 0.2853\n",
      "Epoch [406/882], Train Loss: 0.0001, Valid Loss: 0.3068\n",
      "Epoch [407/882], Train Loss: 0.0001, Valid Loss: 0.3181\n",
      "Epoch [408/882], Train Loss: 0.0000, Valid Loss: 0.3221\n",
      "Epoch [409/882], Train Loss: 0.0000, Valid Loss: 0.3169\n",
      "Epoch [410/882], Train Loss: 0.0000, Valid Loss: 0.3130\n",
      "Epoch [411/882], Train Loss: 0.0012, Valid Loss: 0.2893\n",
      "Epoch [412/882], Train Loss: 0.0036, Valid Loss: 0.2950\n",
      "Epoch [413/882], Train Loss: 0.0060, Valid Loss: 0.2424\n",
      "Epoch [414/882], Train Loss: 0.0027, Valid Loss: 0.3187\n",
      "Epoch [415/882], Train Loss: 0.0001, Valid Loss: 0.3102\n",
      "Epoch [416/882], Train Loss: 0.0001, Valid Loss: 0.3119\n",
      "Epoch [417/882], Train Loss: 0.0001, Valid Loss: 0.3352\n",
      "Epoch [418/882], Train Loss: 0.0005, Valid Loss: 0.3297\n",
      "Epoch [419/882], Train Loss: 0.0001, Valid Loss: 0.2983\n",
      "Epoch [420/882], Train Loss: 0.0002, Valid Loss: 0.2712\n",
      "Epoch [421/882], Train Loss: 0.0000, Valid Loss: 0.2838\n",
      "Epoch [422/882], Train Loss: 0.0000, Valid Loss: 0.2901\n",
      "Epoch [423/882], Train Loss: 0.0064, Valid Loss: 0.2706\n",
      "Epoch [424/882], Train Loss: 0.0015, Valid Loss: 0.2473\n",
      "Epoch [425/882], Train Loss: 0.0015, Valid Loss: 0.2309\n",
      "Epoch [426/882], Train Loss: 0.0002, Valid Loss: 0.2140\n",
      "Epoch [427/882], Train Loss: 0.0010, Valid Loss: 0.2607\n",
      "Epoch [428/882], Train Loss: 0.0001, Valid Loss: 0.2749\n",
      "Epoch [429/882], Train Loss: 0.0000, Valid Loss: 0.2682\n",
      "Epoch [430/882], Train Loss: 0.0020, Valid Loss: 0.1879\n",
      "Epoch [431/882], Train Loss: 0.0001, Valid Loss: 0.1859\n",
      "Epoch [432/882], Train Loss: 0.0005, Valid Loss: 0.2470\n",
      "Epoch [433/882], Train Loss: 0.0001, Valid Loss: 0.2392\n",
      "Epoch [434/882], Train Loss: 0.0000, Valid Loss: 0.2508\n",
      "Epoch [435/882], Train Loss: 0.0001, Valid Loss: 0.3054\n",
      "Epoch [436/882], Train Loss: 0.0000, Valid Loss: 0.3118\n",
      "Epoch [437/882], Train Loss: 0.0000, Valid Loss: 0.3169\n",
      "Epoch [438/882], Train Loss: 0.0000, Valid Loss: 0.3064\n",
      "Epoch [439/882], Train Loss: 0.0000, Valid Loss: 0.2982\n",
      "Epoch [440/882], Train Loss: 0.0000, Valid Loss: 0.2965\n",
      "Epoch [441/882], Train Loss: 0.0001, Valid Loss: 0.3348\n",
      "Epoch [442/882], Train Loss: 0.0000, Valid Loss: 0.3225\n",
      "Epoch [443/882], Train Loss: 0.0003, Valid Loss: 0.2978\n",
      "Epoch [444/882], Train Loss: 0.0028, Valid Loss: 0.4112\n",
      "Epoch [445/882], Train Loss: 0.0013, Valid Loss: 0.5945\n",
      "Epoch [446/882], Train Loss: 0.0001, Valid Loss: 0.4504\n",
      "Epoch [447/882], Train Loss: 0.0002, Valid Loss: 0.6212\n",
      "Epoch [448/882], Train Loss: 0.0000, Valid Loss: 0.4761\n",
      "Epoch [449/882], Train Loss: 0.0000, Valid Loss: 0.3990\n",
      "Epoch [450/882], Train Loss: 0.0000, Valid Loss: 0.3627\n",
      "Epoch [451/882], Train Loss: 0.0001, Valid Loss: 0.3632\n",
      "Epoch [452/882], Train Loss: 0.0000, Valid Loss: 0.5359\n",
      "Epoch [453/882], Train Loss: 0.0003, Valid Loss: 0.4406\n",
      "Epoch [454/882], Train Loss: 0.0002, Valid Loss: 0.3307\n",
      "Epoch [455/882], Train Loss: 0.0000, Valid Loss: 0.4073\n",
      "Epoch [456/882], Train Loss: 0.0000, Valid Loss: 0.4358\n",
      "Epoch [457/882], Train Loss: 0.0000, Valid Loss: 0.4585\n",
      "Epoch [458/882], Train Loss: 0.0000, Valid Loss: 0.4745\n",
      "Epoch [459/882], Train Loss: 0.0004, Valid Loss: 0.8307\n",
      "Epoch [460/882], Train Loss: 0.0000, Valid Loss: 0.8271\n",
      "Epoch [461/882], Train Loss: 0.0001, Valid Loss: 0.5932\n",
      "Epoch [462/882], Train Loss: 0.0011, Valid Loss: 0.5452\n",
      "Epoch [463/882], Train Loss: 0.0006, Valid Loss: 0.7239\n",
      "Epoch [464/882], Train Loss: 0.0003, Valid Loss: 0.4396\n",
      "Epoch [465/882], Train Loss: 0.0009, Valid Loss: 0.8515\n",
      "Epoch [466/882], Train Loss: 0.0002, Valid Loss: 1.1329\n",
      "Epoch [467/882], Train Loss: 0.0001, Valid Loss: 0.8416\n",
      "Epoch [468/882], Train Loss: 0.0000, Valid Loss: 0.7225\n",
      "Epoch [469/882], Train Loss: 0.0001, Valid Loss: 0.5111\n",
      "Epoch [470/882], Train Loss: 0.0000, Valid Loss: 0.4792\n",
      "Epoch [471/882], Train Loss: 0.0000, Valid Loss: 0.4934\n",
      "Epoch [472/882], Train Loss: 0.0000, Valid Loss: 0.4989\n",
      "Epoch [473/882], Train Loss: 0.0000, Valid Loss: 0.5007\n",
      "Epoch [474/882], Train Loss: 0.0000, Valid Loss: 0.5088\n",
      "Epoch [475/882], Train Loss: 0.0010, Valid Loss: 0.4461\n",
      "Epoch [476/882], Train Loss: 0.0020, Valid Loss: 0.3175\n",
      "Epoch [477/882], Train Loss: 0.0028, Valid Loss: 0.4365\n",
      "Epoch [478/882], Train Loss: 0.0020, Valid Loss: 0.5645\n",
      "Epoch [479/882], Train Loss: 0.0191, Valid Loss: 0.3066\n",
      "Epoch [480/882], Train Loss: 0.0027, Valid Loss: 0.4725\n",
      "Epoch [481/882], Train Loss: 0.0005, Valid Loss: 0.4273\n",
      "Epoch [482/882], Train Loss: 0.0007, Valid Loss: 0.3705\n",
      "Epoch [483/882], Train Loss: 0.0002, Valid Loss: 0.3319\n",
      "Epoch [484/882], Train Loss: 0.0000, Valid Loss: 0.3314\n",
      "Epoch [485/882], Train Loss: 0.0000, Valid Loss: 0.3313\n",
      "Epoch [486/882], Train Loss: 0.0001, Valid Loss: 0.3372\n",
      "Epoch [487/882], Train Loss: 0.0000, Valid Loss: 0.3389\n",
      "Epoch [488/882], Train Loss: 0.0000, Valid Loss: 0.3409\n",
      "Epoch [489/882], Train Loss: 0.0000, Valid Loss: 0.3385\n",
      "Epoch [490/882], Train Loss: 0.0000, Valid Loss: 0.3411\n",
      "Epoch [491/882], Train Loss: 0.0001, Valid Loss: 0.3642\n",
      "Epoch [492/882], Train Loss: 0.0000, Valid Loss: 0.3647\n",
      "Epoch [493/882], Train Loss: 0.0000, Valid Loss: 0.3684\n",
      "Epoch [494/882], Train Loss: 0.0019, Valid Loss: 0.2709\n",
      "Epoch [495/882], Train Loss: 0.0001, Valid Loss: 0.2520\n",
      "Epoch [496/882], Train Loss: 0.0001, Valid Loss: 0.3672\n",
      "Epoch [497/882], Train Loss: 0.0006, Valid Loss: 0.5601\n",
      "Epoch [498/882], Train Loss: 0.0001, Valid Loss: 0.6160\n",
      "Epoch [499/882], Train Loss: 0.0001, Valid Loss: 0.4882\n",
      "Epoch [500/882], Train Loss: 0.0000, Valid Loss: 0.4615\n",
      "Epoch [501/882], Train Loss: 0.0002, Valid Loss: 0.4297\n",
      "Epoch [502/882], Train Loss: 0.0000, Valid Loss: 0.3217\n",
      "Epoch [503/882], Train Loss: 0.0003, Valid Loss: 0.5194\n",
      "Epoch [504/882], Train Loss: 0.0000, Valid Loss: 0.4941\n",
      "Epoch [505/882], Train Loss: 0.0079, Valid Loss: 0.4643\n",
      "Epoch [506/882], Train Loss: 0.0030, Valid Loss: 0.3229\n",
      "Epoch [507/882], Train Loss: 0.0001, Valid Loss: 0.4850\n",
      "Epoch [508/882], Train Loss: 0.0002, Valid Loss: 0.2919\n",
      "Epoch [509/882], Train Loss: 0.0001, Valid Loss: 0.2242\n",
      "Epoch [510/882], Train Loss: 0.0001, Valid Loss: 0.2115\n",
      "Epoch [511/882], Train Loss: 0.0000, Valid Loss: 0.2064\n",
      "Epoch [512/882], Train Loss: 0.0000, Valid Loss: 0.2091\n",
      "Epoch [513/882], Train Loss: 0.0000, Valid Loss: 0.2015\n",
      "Epoch [514/882], Train Loss: 0.0000, Valid Loss: 0.2037\n",
      "Epoch [515/882], Train Loss: 0.0000, Valid Loss: 0.2056\n",
      "Epoch [516/882], Train Loss: 0.0000, Valid Loss: 0.2010\n",
      "Epoch [517/882], Train Loss: 0.0000, Valid Loss: 0.2023\n",
      "Epoch [518/882], Train Loss: 0.0000, Valid Loss: 0.1986\n",
      "Epoch [519/882], Train Loss: 0.0002, Valid Loss: 0.1725\n",
      "Epoch [520/882], Train Loss: 0.0000, Valid Loss: 0.1637\n",
      "Epoch [521/882], Train Loss: 0.0001, Valid Loss: 0.1844\n",
      "Epoch [522/882], Train Loss: 0.0000, Valid Loss: 0.1908\n",
      "Epoch [523/882], Train Loss: 0.0000, Valid Loss: 0.1894\n",
      "Epoch [524/882], Train Loss: 0.0000, Valid Loss: 0.1892\n",
      "Epoch [525/882], Train Loss: 0.0000, Valid Loss: 0.1860\n",
      "Epoch [526/882], Train Loss: 0.0001, Valid Loss: 0.1942\n",
      "Epoch [527/882], Train Loss: 0.0000, Valid Loss: 0.2044\n",
      "Epoch [528/882], Train Loss: 0.0000, Valid Loss: 0.2050\n",
      "Epoch [529/882], Train Loss: 0.0000, Valid Loss: 0.2033\n",
      "Epoch [530/882], Train Loss: 0.0000, Valid Loss: 0.1921\n",
      "Epoch [531/882], Train Loss: 0.0000, Valid Loss: 0.2021\n",
      "Epoch [532/882], Train Loss: 0.0000, Valid Loss: 0.2025\n",
      "Epoch [533/882], Train Loss: 0.0000, Valid Loss: 0.2014\n",
      "Epoch [534/882], Train Loss: 0.0000, Valid Loss: 0.2062\n",
      "Epoch [535/882], Train Loss: 0.0000, Valid Loss: 0.2056\n",
      "Epoch [536/882], Train Loss: 0.0000, Valid Loss: 0.2078\n",
      "Epoch [537/882], Train Loss: 0.0000, Valid Loss: 0.2051\n",
      "Epoch [538/882], Train Loss: 0.0000, Valid Loss: 0.2016\n",
      "Epoch [539/882], Train Loss: 0.0000, Valid Loss: 0.2028\n",
      "Epoch [540/882], Train Loss: 0.0000, Valid Loss: 0.2108\n",
      "Epoch [541/882], Train Loss: 0.0000, Valid Loss: 0.2115\n",
      "Epoch [542/882], Train Loss: 0.0000, Valid Loss: 0.2095\n",
      "Epoch [543/882], Train Loss: 0.0000, Valid Loss: 0.2087\n",
      "Epoch [544/882], Train Loss: 0.0000, Valid Loss: 0.2065\n",
      "Epoch [545/882], Train Loss: 0.0000, Valid Loss: 0.1970\n",
      "Epoch [546/882], Train Loss: 0.0000, Valid Loss: 0.1936\n",
      "Epoch [547/882], Train Loss: 0.0000, Valid Loss: 0.1941\n",
      "Epoch [548/882], Train Loss: 0.0000, Valid Loss: 0.1944\n",
      "Epoch [549/882], Train Loss: 0.0000, Valid Loss: 0.1951\n",
      "Epoch [550/882], Train Loss: 0.0000, Valid Loss: 0.1981\n",
      "Epoch [551/882], Train Loss: 0.0000, Valid Loss: 0.2000\n",
      "Epoch [552/882], Train Loss: 0.0000, Valid Loss: 0.2021\n",
      "Epoch [553/882], Train Loss: 0.0000, Valid Loss: 0.1872\n",
      "Epoch [554/882], Train Loss: 0.0000, Valid Loss: 0.2010\n",
      "Epoch [555/882], Train Loss: 0.0000, Valid Loss: 0.1908\n",
      "Epoch [556/882], Train Loss: 0.0000, Valid Loss: 0.1842\n",
      "Epoch [557/882], Train Loss: 0.0000, Valid Loss: 0.1801\n",
      "Epoch [558/882], Train Loss: 0.0000, Valid Loss: 0.1796\n",
      "Epoch [559/882], Train Loss: 0.0000, Valid Loss: 0.1500\n",
      "Epoch [560/882], Train Loss: 0.0000, Valid Loss: 0.1563\n",
      "Epoch [561/882], Train Loss: 0.0000, Valid Loss: 0.1842\n",
      "Epoch [562/882], Train Loss: 0.0000, Valid Loss: 0.1807\n",
      "Epoch [563/882], Train Loss: 0.0000, Valid Loss: 0.1941\n",
      "Epoch [564/882], Train Loss: 0.0000, Valid Loss: 0.2036\n",
      "Epoch [565/882], Train Loss: 0.0000, Valid Loss: 0.2116\n",
      "Epoch [566/882], Train Loss: 0.0000, Valid Loss: 0.2182\n",
      "Epoch [567/882], Train Loss: 0.0000, Valid Loss: 0.2168\n",
      "Epoch [568/882], Train Loss: 0.0000, Valid Loss: 0.2209\n",
      "Epoch [569/882], Train Loss: 0.0000, Valid Loss: 0.2253\n",
      "Epoch [570/882], Train Loss: 0.0000, Valid Loss: 0.2266\n",
      "Epoch [571/882], Train Loss: 0.0000, Valid Loss: 0.2309\n",
      "Epoch [572/882], Train Loss: 0.0000, Valid Loss: 0.2286\n",
      "Epoch [573/882], Train Loss: 0.0000, Valid Loss: 0.2262\n",
      "Epoch [574/882], Train Loss: 0.0000, Valid Loss: 0.2235\n",
      "Epoch [575/882], Train Loss: 0.0000, Valid Loss: 0.2271\n",
      "Epoch [576/882], Train Loss: 0.0000, Valid Loss: 0.2268\n",
      "Epoch [577/882], Train Loss: 0.0000, Valid Loss: 0.2283\n",
      "Epoch [578/882], Train Loss: 0.0000, Valid Loss: 0.2300\n",
      "Epoch [579/882], Train Loss: 0.0000, Valid Loss: 0.2348\n",
      "Epoch [580/882], Train Loss: 0.0000, Valid Loss: 0.2390\n",
      "Epoch [581/882], Train Loss: 0.0000, Valid Loss: 0.2382\n",
      "Epoch [582/882], Train Loss: 0.0000, Valid Loss: 0.2346\n",
      "Epoch [583/882], Train Loss: 0.0000, Valid Loss: 0.2351\n",
      "Epoch [584/882], Train Loss: 0.0000, Valid Loss: 0.2376\n",
      "Epoch [585/882], Train Loss: 0.0000, Valid Loss: 0.2386\n",
      "Epoch [586/882], Train Loss: 0.0000, Valid Loss: 0.2359\n",
      "Epoch [587/882], Train Loss: 0.0000, Valid Loss: 0.2398\n",
      "Epoch [588/882], Train Loss: 0.0000, Valid Loss: 0.2391\n",
      "Epoch [589/882], Train Loss: 0.0000, Valid Loss: 0.3668\n",
      "Epoch [590/882], Train Loss: 0.0082, Valid Loss: 1.0935\n",
      "Epoch [591/882], Train Loss: 0.0026, Valid Loss: 0.6910\n",
      "Epoch [592/882], Train Loss: 0.0196, Valid Loss: 0.3533\n",
      "Epoch [593/882], Train Loss: 0.0021, Valid Loss: 0.4437\n",
      "Epoch [594/882], Train Loss: 0.0005, Valid Loss: 0.4109\n",
      "Epoch [595/882], Train Loss: 0.0004, Valid Loss: 0.2284\n",
      "Epoch [596/882], Train Loss: 0.0001, Valid Loss: 0.2820\n",
      "Epoch [597/882], Train Loss: 0.0001, Valid Loss: 0.3592\n",
      "Epoch [598/882], Train Loss: 0.0000, Valid Loss: 0.3767\n",
      "Epoch [599/882], Train Loss: 0.0000, Valid Loss: 0.3811\n",
      "Epoch [600/882], Train Loss: 0.0000, Valid Loss: 0.3845\n",
      "Epoch [601/882], Train Loss: 0.0009, Valid Loss: 0.5120\n",
      "Epoch [602/882], Train Loss: 0.0011, Valid Loss: 0.6124\n",
      "Epoch [603/882], Train Loss: 0.0015, Valid Loss: 0.7471\n",
      "Epoch [604/882], Train Loss: 0.0000, Valid Loss: 0.6099\n",
      "Epoch [605/882], Train Loss: 0.0000, Valid Loss: 0.5191\n",
      "Epoch [606/882], Train Loss: 0.0000, Valid Loss: 0.4705\n",
      "Epoch [607/882], Train Loss: 0.0000, Valid Loss: 0.4363\n",
      "Epoch [608/882], Train Loss: 0.0000, Valid Loss: 0.4132\n",
      "Epoch [609/882], Train Loss: 0.0000, Valid Loss: 0.3879\n",
      "Epoch [610/882], Train Loss: 0.0000, Valid Loss: 0.3733\n",
      "Epoch [611/882], Train Loss: 0.0000, Valid Loss: 0.3663\n",
      "Epoch [612/882], Train Loss: 0.0000, Valid Loss: 0.3640\n",
      "Epoch [613/882], Train Loss: 0.0000, Valid Loss: 0.3645\n",
      "Epoch [614/882], Train Loss: 0.0000, Valid Loss: 0.3589\n",
      "Epoch [615/882], Train Loss: 0.0000, Valid Loss: 0.3599\n",
      "Epoch [616/882], Train Loss: 0.0012, Valid Loss: 0.3982\n",
      "Epoch [617/882], Train Loss: 0.0212, Valid Loss: 0.3714\n",
      "Epoch [618/882], Train Loss: 0.0004, Valid Loss: 0.3037\n",
      "Epoch [619/882], Train Loss: 0.0002, Valid Loss: 0.3026\n",
      "Epoch [620/882], Train Loss: 0.0004, Valid Loss: 0.2073\n",
      "Epoch [621/882], Train Loss: 0.0000, Valid Loss: 0.2294\n",
      "Epoch [622/882], Train Loss: 0.0000, Valid Loss: 0.2345\n",
      "Epoch [623/882], Train Loss: 0.0000, Valid Loss: 0.2413\n",
      "Epoch [624/882], Train Loss: 0.0001, Valid Loss: 0.2724\n",
      "Epoch [625/882], Train Loss: 0.0002, Valid Loss: 0.1970\n",
      "Epoch [626/882], Train Loss: 0.0000, Valid Loss: 0.1988\n",
      "Epoch [627/882], Train Loss: 0.0018, Valid Loss: 0.2631\n",
      "Epoch [628/882], Train Loss: 0.0002, Valid Loss: 0.3060\n",
      "Epoch [629/882], Train Loss: 0.0000, Valid Loss: 0.2957\n",
      "Epoch [630/882], Train Loss: 0.0001, Valid Loss: 0.2561\n",
      "Epoch [631/882], Train Loss: 0.0000, Valid Loss: 0.2567\n",
      "Epoch [632/882], Train Loss: 0.0000, Valid Loss: 0.2621\n",
      "Epoch [633/882], Train Loss: 0.0000, Valid Loss: 0.2597\n",
      "Epoch [634/882], Train Loss: 0.0000, Valid Loss: 0.2588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-04-16 16:50:05,839] Trial 0 failed with parameters: {'learning_rate': 0.004581446189538255, 'optimizer': 'Adamax', 'num_epochs': 882} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_860080/3534238321.py\", line 96, in objective\n",
      "    return train_model(trial, train_loader, val_loader)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_860080/3534238321.py\", line 46, in train_model\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/tmp/ipykernel_860080/1544742987.py\", line 28, in __getitem__\n",
      "    image = Image.open(full_img_path).convert(\"RGB\")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/PIL/Image.py\", line 937, in convert\n",
      "    self.load()\n",
      "  File \"/home/olarinoyem/miniconda3/envs/deep_tf/lib/python3.11/site-packages/PIL/ImageFile.py\", line 269, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-04-16 16:50:05,842] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Create a study object and optimize hyperparameters\u001b[39;00m\n\u001b[1;32m     99\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters and train the final model with them\u001b[39;00m\n\u001b[1;32m    103\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# You can pass additional arguments to `train_model` if needed\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(trial, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m     45\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Move data to the device\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx]\n\u001b[1;32m     27\u001b[0m full_img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, img_name)\n\u001b[0;32m---> 28\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_img_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m frame_identifier \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(img_name)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_map\u001b[38;5;241m.\u001b[39mget(frame_identifier)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/PIL/Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    891\u001b[0m ):\n\u001b[1;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_tf/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have torchvision installed\n",
    "# from torchvision.models import EfficientNet\n",
    "import optuna\n",
    "\n",
    "# Check for GPU availability and use it if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define the training function\n",
    "def train_model(trial, train_loader, val_loader, num_epochs=5):\n",
    "    # Sample hyperparameters from the trial\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-7, 0.1)\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "        \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adamax\", \"Adagrad\", \"Adadelta\"]\n",
    "    )\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 500, 1500)\n",
    "\n",
    "    # Initialize and move the model to the specified device\n",
    "    model = EfficientNet.from_name(\"efficientnet-b0\", num_classes=2).to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define optimizer based on the sampled name\n",
    "    optimizer = {\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=learning_rate),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=learning_rate),\n",
    "        \"Adamax\": optim.Adamax(model.parameters(), lr=learning_rate),\n",
    "        \"Adagrad\": optim.Adagrad(model.parameters(), lr=learning_rate),\n",
    "        \"Adadelta\": optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    }[optimizer_name]\n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move data to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        valid_loss /= len(val_loader)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Report intermediate results to Optuna\n",
    "        trial.report(valid_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Print training statistics\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return valid_losses[-1]\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # You can pass additional arguments to `train_model` if needed\n",
    "    return train_model(trial, train_loader, val_loader)\n",
    "\n",
    "# Create a study object and optimize hyperparameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=17)\n",
    "\n",
    "# Get the best hyperparameters and train the final model with them\n",
    "best_params = study.best_params\n",
    "# final_val_loss = train_model(best_params, train_loader, val_loader, num_epochs=1500)\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "# print(\"Final validation loss with best hyperparameters:\", final_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming 'study' is your Optuna study object\n",
    "with open(\"study.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
