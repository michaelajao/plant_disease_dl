{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986a0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_baseline_models.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "from helper_functions import *\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='baseline_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-3     # Adjust as necessary\n",
    "NUM_EPOCHS = 50          # Adjust based on experimentation\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_Baselines\"\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \".\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"baseline_models\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Baseline Model Definitions\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline Model Definition\n",
    "# ------------------------------\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a simple baseline model with fewer layers.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Define fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * (HEIGHT // 2) * (WIDTH // 2),\n",
    "                out_features=output_shape,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)  # Convolutional layers\n",
    "        x = self.classifier(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# ConvNetPlus Model Definition\n",
    "# ------------------------------\n",
    "class ConvNetPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines an improved model with additional layers, batch normalization, and dropout.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(ConvNetPlus, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 2 * (HEIGHT // 4) * (WIDTH // 4),\n",
    "                out_features=hidden_units * 4,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units * 4, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# TinyVGG Model Definition\n",
    "# ------------------------------\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a TinyVGG model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(TinyVGG, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units * 2,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 2 * (HEIGHT // 4) * (WIDTH // 4),\n",
    "                out_features=output_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ================================================================\n",
    "# EfficientNetV2 Model Definition\n",
    "# ================================================================\n",
    "\n",
    "def get_efficientnetv2_model(output_size: int):\n",
    "    \"\"\"\n",
    "    Instantiates an EfficientNetV2 model with pretrained weights.\n",
    "\n",
    "    Args:\n",
    "        output_size (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: EfficientNetV2 model.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        \"efficientnetv2_rw_s\",  # Using EfficientNetV2 RW small version\n",
    "        pretrained=True,         # Use pretrained weights\n",
    "        num_classes=output_size, # Adjust output size to match number of classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ================================================================\n",
    "# Instantiate Models and Optimizers\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Instantiate models\n",
    "baseline_model = BaselineModel(input_shape=3, hidden_units=10, output_shape=output_size)\n",
    "convnetplus_model = ConvNetPlus(input_shape=3, hidden_units=32, output_shape=output_size)\n",
    "tinyvgg_model = TinyVGG(input_shape=3, hidden_units=64, output_shape=output_size)\n",
    "efficientnetv2_model = get_efficientnetv2_model(output_size=output_size)\n",
    "\n",
    "# Move models to device\n",
    "baseline_model = baseline_model.to(device)\n",
    "convnetplus_model = convnetplus_model.to(device)\n",
    "tinyvgg_model = tinyvgg_model.to(device)\n",
    "efficientnetv2_model = efficientnetv2_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    baseline_model = nn.DataParallel(baseline_model)\n",
    "    convnetplus_model = nn.DataParallel(convnetplus_model)\n",
    "    tinyvgg_model = nn.DataParallel(tinyvgg_model)\n",
    "    efficientnetv2_model = nn.DataParallel(efficientnetv2_model)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers for each model\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n",
    "convnetplus_optimizer = optim.SGD(convnetplus_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "tinyvgg_optimizer = optim.RMSprop(tinyvgg_model.parameters(), lr=LEARNING_RATE)\n",
    "efficientnetv2_optimizer = optim.Adam(\n",
    "    efficientnetv2_model.parameters(), lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Learning rate schedulers\n",
    "baseline_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    baseline_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "convnetplus_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    convnetplus_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "tinyvgg_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    tinyvgg_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "efficientnetv2_scheduler = optim.lr_scheduler.StepLR(\n",
    "    efficientnetv2_optimizer, step_size=5, gamma=0.1, verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device, scaler, epoch, model_name, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f\"{model_name} - Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = dict(zip(unique, counts))\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"{model_name} - Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device, model_name, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Weights & Biases (W&B)\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"Baseline_Models_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "        \"class_imbalance_handling\": \"WeightedRandomSampler\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop for Baseline Models\n",
    "# ================================================================\n",
    "\n",
    "def train_model(model, model_name, train_loader, valid_loader, loss_fn, optimizer, scheduler, device, output_dirs, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Trains and validates a given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        model_name (str): Name of the model for identification.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        valid_loader (DataLoader): Validation data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        device (torch.device): Device to train on.\n",
    "        output_dirs (list): List of output directories for saving models and figures.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=EARLY_STOPPING_PATIENCE, \n",
    "        verbose=True, \n",
    "        path=os.path.join(output_dirs[2], f\"{model_name}_early_stop_model.pth\")\n",
    "    )\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        path=os.path.join(output_dirs[2], f\"{model_name}_best_val_loss_model.pth\"), \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Initialize GradScaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Time tracking\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{model_name} - Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training Phase\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        print(f\"{model_name} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        \n",
    "        # Validation Phase\n",
    "        val_loss, val_acc, _, _ = validate(\n",
    "            model=model,\n",
    "            dataloader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        print(f\"{model_name} - Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        print(f\"{model_name} - Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "        \n",
    "        # Append metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Log learning rate\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({f\"{model_name}/learning_rate\": current_lr})\n",
    "        \n",
    "        # Model checkpoint based on validation loss\n",
    "        model_checkpoint(val_loss, model)\n",
    "        \n",
    "        # Early Stopping based on validation loss\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"{model_name} - Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    print(f\"\\n{model_name} - Total Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "    # Log total training time to W&B\n",
    "    wandb.log({f\"{model_name}/total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "    # Plot training metrics\n",
    "    plot_save_path = os.path.join(output_dirs[1], f\"{model_name}_training_validation_metrics.png\")\n",
    "    plot_training_metrics(\n",
    "        train_losses, \n",
    "        train_accuracies, \n",
    "        val_losses, \n",
    "        val_accuracies, \n",
    "        model_name=model_name,\n",
    "        save_path=plot_save_path\n",
    "    )\n",
    "\n",
    "    # Perform post-training evaluation on the validation set\n",
    "    evaluate_model_post_training(\n",
    "        model=model, \n",
    "        dataloader=valid_loader, \n",
    "        device=device, \n",
    "        idx_to_disease=idx_to_disease, \n",
    "        model_name=model_name, \n",
    "        save_dir=output_dirs[1]\n",
    "    )\n",
    "\n",
    "# ================================================================\n",
    "# Main Execution\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of models to train\n",
    "    models = [\n",
    "        {\n",
    "            \"model\": baseline_model,\n",
    "            \"name\": \"BaselineModel\",\n",
    "            \"optimizer\": baseline_optimizer,\n",
    "            \"scheduler\": baseline_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": convnetplus_model,\n",
    "            \"name\": \"ConvNetPlus\",\n",
    "            \"optimizer\": convnetplus_optimizer,\n",
    "            \"scheduler\": convnetplus_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": tinyvgg_model,\n",
    "            \"name\": \"TinyVGG\",\n",
    "            \"optimizer\": tinyvgg_optimizer,\n",
    "            \"scheduler\": tinyvgg_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": efficientnetv2_model,\n",
    "            \"name\": \"EfficientNetV2\",\n",
    "            \"optimizer\": efficientnetv2_optimizer,\n",
    "            \"scheduler\": efficientnetv2_scheduler\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for m in models:\n",
    "        print(f\"\\n{'='*50}\\nStarting Training for {m['name']}\\n{'='*50}\")\n",
    "        wandb.run.name = f\"{m['name']}_Training\"\n",
    "        wandb.run.save()\n",
    "        train_model(\n",
    "            model=m[\"model\"],\n",
    "            model_name=m[\"name\"],\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=m[\"optimizer\"],\n",
    "            scheduler=m[\"scheduler\"],\n",
    "            device=device,\n",
    "            output_dirs=output_dirs,\n",
    "            num_epochs=NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "    # Finalize W&B run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c51a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olarinoyem/Research/plant_disease_dl/src/models/wandb/run-20241021_091748-6a1hm09l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_Baselines/runs/6a1hm09l' target=\"_blank\">Baseline_Models_Training</a></strong> to <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_Baselines' target=\"_blank\">https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_Baselines/runs/6a1hm09l' target=\"_blank\">https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_Baselines/runs/6a1hm09l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_baseline_models.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "from helper_functions import *\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='baseline_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-3     # Adjust as necessary\n",
    "NUM_EPOCHS = 50          # Adjust based on experimentation\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_Baselines\"\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"baseline_models\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Baseline Model Definitions\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline Model Definition\n",
    "# ------------------------------\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a simple baseline model with fewer layers.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Define fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * (HEIGHT // 2) * (WIDTH // 2),\n",
    "                out_features=output_shape,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)  # Convolutional layers\n",
    "        x = self.classifier(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# ConvNetPlus Model Definition\n",
    "# ------------------------------\n",
    "class ConvNetPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines an improved model with additional layers, batch normalization, and dropout.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(ConvNetPlus, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 2 * (HEIGHT // 4) * (WIDTH // 4),\n",
    "                out_features=hidden_units * 4,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units * 4, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# TinyVGG Model Definition\n",
    "# ------------------------------\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a TinyVGG model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(TinyVGG, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units * 2,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 2 * (HEIGHT // 4) * (WIDTH // 4),\n",
    "                out_features=output_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ================================================================\n",
    "# EfficientNetV2 Model Definition\n",
    "# ================================================================\n",
    "\n",
    "def get_efficientnetv2_model(output_size: int):\n",
    "    \"\"\"\n",
    "    Instantiates an EfficientNetV2 model with pretrained weights.\n",
    "\n",
    "    Args:\n",
    "        output_size (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: EfficientNetV2 model.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        \"efficientnetv2_rw_s\",  # Using EfficientNetV2 RW small version\n",
    "        pretrained=True,         # Use pretrained weights\n",
    "        num_classes=output_size, # Adjust output size to match number of classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ================================================================\n",
    "# Instantiate Models and Optimizers\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Instantiate models\n",
    "baseline_model = BaselineModel(input_shape=3, hidden_units=10, output_shape=output_size)\n",
    "convnetplus_model = ConvNetPlus(input_shape=3, hidden_units=32, output_shape=output_size)\n",
    "tinyvgg_model = TinyVGG(input_shape=3, hidden_units=64, output_shape=output_size)\n",
    "efficientnetv2_model = get_efficientnetv2_model(output_size=output_size)\n",
    "\n",
    "# Move models to device\n",
    "baseline_model = baseline_model.to(device)\n",
    "convnetplus_model = convnetplus_model.to(device)\n",
    "tinyvgg_model = tinyvgg_model.to(device)\n",
    "efficientnetv2_model = efficientnetv2_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    baseline_model = nn.DataParallel(baseline_model)\n",
    "    convnetplus_model = nn.DataParallel(convnetplus_model)\n",
    "    tinyvgg_model = nn.DataParallel(tinyvgg_model)\n",
    "    efficientnetv2_model = nn.DataParallel(efficientnetv2_model)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers for each model\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n",
    "convnetplus_optimizer = optim.SGD(convnetplus_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "tinyvgg_optimizer = optim.RMSprop(tinyvgg_model.parameters(), lr=LEARNING_RATE)\n",
    "efficientnetv2_optimizer = optim.Adam(\n",
    "    efficientnetv2_model.parameters(), lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Learning rate schedulers\n",
    "baseline_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    baseline_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "convnetplus_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    convnetplus_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "tinyvgg_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    tinyvgg_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "efficientnetv2_scheduler = optim.lr_scheduler.StepLR(\n",
    "    efficientnetv2_optimizer, step_size=5, gamma=0.1, verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device, scaler, epoch, model_name, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f\"{model_name} - Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = dict(zip(unique, counts))\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"{model_name} - Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device, model_name, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Weights & Biases (W&B)\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"Baseline_Models_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "        \"class_imbalance_handling\": \"WeightedRandomSampler\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop for Baseline Models\n",
    "# ================================================================\n",
    "\n",
    "def train_model(model, model_name, train_loader, valid_loader, loss_fn, optimizer, scheduler, device, output_dirs, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Trains and validates a given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        model_name (str): Name of the model for identification.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        valid_loader (DataLoader): Validation data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        device (torch.device): Device to train on.\n",
    "        output_dirs (list): List of output directories for saving models and figures.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=EARLY_STOPPING_PATIENCE, \n",
    "        verbose=True, \n",
    "        path=os.path.join(output_dirs[2], f\"{model_name}_early_stop_model.pth\")\n",
    "    )\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        path=os.path.join(output_dirs[2], f\"{model_name}_best_val_loss_model.pth\"), \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Initialize GradScaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Time tracking\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{model_name} - Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training Phase\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        print(f\"{model_name} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        \n",
    "        # Validation Phase\n",
    "        val_loss, val_acc, _, _ = validate(\n",
    "            model=model,\n",
    "            dataloader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        print(f\"{model_name} - Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        print(f\"{model_name} - Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "        \n",
    "        # Append metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Log learning rate\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({f\"{model_name}/learning_rate\": current_lr})\n",
    "        \n",
    "        # Model checkpoint based on validation loss\n",
    "        model_checkpoint(val_loss, model)\n",
    "        \n",
    "        # Early Stopping based on validation loss\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"{model_name} - Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    print(f\"\\n{model_name} - Total Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "    # Log total training time to W&B\n",
    "    wandb.log({f\"{model_name}/total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "    # Plot training metrics\n",
    "    plot_save_path = os.path.join(output_dirs[1], f\"{model_name}_training_validation_metrics.png\")\n",
    "    plot_training_metrics(\n",
    "        train_losses, \n",
    "        train_accuracies, \n",
    "        val_losses, \n",
    "        val_accuracies, \n",
    "        model_name=model_name,\n",
    "        save_path=plot_save_path\n",
    "    )\n",
    "\n",
    "    # Perform post-training evaluation on the validation set\n",
    "    evaluate_model_post_training(\n",
    "        model=model, \n",
    "        dataloader=valid_loader, \n",
    "        device=device, \n",
    "        idx_to_disease=idx_to_disease, \n",
    "        model_name=model_name, \n",
    "        save_dir=output_dirs[1]\n",
    "    )\n",
    "\n",
    "# ================================================================\n",
    "# Main Execution\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of models to train\n",
    "    models = [\n",
    "        {\n",
    "            \"model\": baseline_model,\n",
    "            \"name\": \"BaselineModel\",\n",
    "            \"optimizer\": baseline_optimizer,\n",
    "            \"scheduler\": baseline_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": convnetplus_model,\n",
    "            \"name\": \"ConvNetPlus\",\n",
    "            \"optimizer\": convnetplus_optimizer,\n",
    "            \"scheduler\": convnetplus_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": tinyvgg_model,\n",
    "            \"name\": \"TinyVGG\",\n",
    "            \"optimizer\": tinyvgg_optimizer,\n",
    "            \"scheduler\": tinyvgg_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": efficientnetv2_model,\n",
    "            \"name\": \"EfficientNetV2\",\n",
    "            \"optimizer\": efficientnetv2_optimizer,\n",
    "            \"scheduler\": efficientnetv2_scheduler\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for m in models:\n",
    "        print(f\"\\n{'='*50}\\nStarting Training for {m['name']}\\n{'='*50}\")\n",
    "        wandb.run.name = f\"{m['name']}_Training\"\n",
    "        wandb.run.save()\n",
    "        train_model(\n",
    "            model=m[\"model\"],\n",
    "            model_name=m[\"name\"],\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=m[\"optimizer\"],\n",
    "            scheduler=m[\"scheduler\"],\n",
    "            device=device,\n",
    "            output_dirs=output_dirs,\n",
    "            num_epochs=NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "    # Finalize W&B run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b438c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_baseline_models.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "from helper_functions import *\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='baseline_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-3     # Adjust as necessary\n",
    "NUM_EPOCHS = 50          # Adjust based on experimentation\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_Baselines\"\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"baseline_models\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Baseline Model Definitions\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------\n",
    "# Baseline Model Definition\n",
    "# ------------------------------\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a simple baseline model with fewer layers.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Define fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * (HEIGHT // 2) * (WIDTH // 2),\n",
    "                out_features=output_shape,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)  # Convolutional layers\n",
    "        x = self.classifier(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# ConvNetPlus Model Definition\n",
    "# ------------------------------\n",
    "class ConvNetPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines an improved model with additional layers, batch normalization, and dropout.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(ConvNetPlus, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 2 * (HEIGHT // 4) * (WIDTH // 4),\n",
    "                out_features=hidden_units * 4,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units * 4, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# TinyVGG Model Definition\n",
    "# ------------------------------\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a TinyVGG model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): Number of input channels.\n",
    "        hidden_units (int): Number of units in the hidden layers.\n",
    "        output_shape (int): Number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super(TinyVGG, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units * 2,\n",
    "                out_channels=hidden_units * 2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 2 * (HEIGHT // 4) * (WIDTH // 4),\n",
    "                out_features=output_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ================================================================\n",
    "# EfficientNetV2 Model Definition\n",
    "# ================================================================\n",
    "\n",
    "def get_efficientnetv2_model(output_size: int):\n",
    "    \"\"\"\n",
    "    Instantiates an EfficientNetV2 model with pretrained weights.\n",
    "\n",
    "    Args:\n",
    "        output_size (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: EfficientNetV2 model.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        \"efficientnetv2_rw_s\",  # Using EfficientNetV2 RW small version\n",
    "        pretrained=True,         # Use pretrained weights\n",
    "        num_classes=output_size, # Adjust output size to match number of classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ================================================================\n",
    "# Instantiate Models and Optimizers\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Instantiate models\n",
    "baseline_model = BaselineModel(input_shape=3, hidden_units=10, output_shape=output_size)\n",
    "convnetplus_model = ConvNetPlus(input_shape=3, hidden_units=32, output_shape=output_size)\n",
    "tinyvgg_model = TinyVGG(input_shape=3, hidden_units=64, output_shape=output_size)\n",
    "efficientnetv2_model = get_efficientnetv2_model(output_size=output_size)\n",
    "\n",
    "# Move models to device\n",
    "baseline_model = baseline_model.to(device)\n",
    "convnetplus_model = convnetplus_model.to(device)\n",
    "tinyvgg_model = tinyvgg_model.to(device)\n",
    "efficientnetv2_model = efficientnetv2_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    baseline_model = nn.DataParallel(baseline_model)\n",
    "    convnetplus_model = nn.DataParallel(convnetplus_model)\n",
    "    tinyvgg_model = nn.DataParallel(tinyvgg_model)\n",
    "    efficientnetv2_model = nn.DataParallel(efficientnetv2_model)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers for each model\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n",
    "convnetplus_optimizer = optim.SGD(convnetplus_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "tinyvgg_optimizer = optim.RMSprop(tinyvgg_model.parameters(), lr=LEARNING_RATE)\n",
    "efficientnetv2_optimizer = optim.Adam(\n",
    "    efficientnetv2_model.parameters(), lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Learning rate schedulers\n",
    "baseline_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    baseline_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "convnetplus_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    convnetplus_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "tinyvgg_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    tinyvgg_optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "efficientnetv2_scheduler = optim.lr_scheduler.StepLR(\n",
    "    efficientnetv2_optimizer, step_size=5, gamma=0.1, verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device, scaler, epoch, model_name, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f\"{model_name} - Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = dict(zip(unique, counts))\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"{model_name} - Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device, model_name, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Weights & Biases (W&B)\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"Baseline_Models_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "        \"class_imbalance_handling\": \"WeightedRandomSampler\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop for Baseline Models\n",
    "# ================================================================\n",
    "\n",
    "def train_model(model, model_name, train_loader, valid_loader, loss_fn, optimizer, scheduler, device, output_dirs, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Trains and validates a given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        model_name (str): Name of the model for identification.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        valid_loader (DataLoader): Validation data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        device (torch.device): Device to train on.\n",
    "        output_dirs (list): List of output directories for saving models and figures.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=EARLY_STOPPING_PATIENCE, \n",
    "        verbose=True, \n",
    "        path=os.path.join(output_dirs[2], f\"{model_name}_early_stop_model.pth\")\n",
    "    )\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        path=os.path.join(output_dirs[2], f\"{model_name}_best_val_loss_model.pth\"), \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Initialize GradScaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Time tracking\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{model_name} - Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training Phase\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        print(f\"{model_name} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        \n",
    "        # Validation Phase\n",
    "        val_loss, val_acc, _, _ = validate(\n",
    "            model=model,\n",
    "            dataloader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        print(f\"{model_name} - Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        print(f\"{model_name} - Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "        \n",
    "        # Append metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Log learning rate\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        else:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({f\"{model_name}/learning_rate\": current_lr})\n",
    "        \n",
    "        # Model checkpoint based on validation loss\n",
    "        model_checkpoint(val_loss, model)\n",
    "        \n",
    "        # Early Stopping based on validation loss\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"{model_name} - Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    print(f\"\\n{model_name} - Total Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "    # Log total training time to W&B\n",
    "    wandb.log({f\"{model_name}/total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "    # Plot training metrics\n",
    "    plot_save_path = os.path.join(output_dirs[1], f\"{model_name}_training_validation_metrics.png\")\n",
    "    plot_training_metrics(\n",
    "        train_losses, \n",
    "        train_accuracies, \n",
    "        val_losses, \n",
    "        val_accuracies, \n",
    "        model_name=model_name,\n",
    "        save_path=plot_save_path\n",
    "    )\n",
    "\n",
    "    # Perform post-training evaluation on the validation set\n",
    "    evaluate_model_post_training(\n",
    "        model=model, \n",
    "        dataloader=valid_loader, \n",
    "        device=device, \n",
    "        idx_to_disease=idx_to_disease, \n",
    "        model_name=model_name, \n",
    "        save_dir=output_dirs[1]\n",
    "    )\n",
    "\n",
    "# ================================================================\n",
    "# Main Execution\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of models to train\n",
    "    models = [\n",
    "        {\n",
    "            \"model\": baseline_model,\n",
    "            \"name\": \"BaselineModel\",\n",
    "            \"optimizer\": baseline_optimizer,\n",
    "            \"scheduler\": baseline_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": convnetplus_model,\n",
    "            \"name\": \"ConvNetPlus\",\n",
    "            \"optimizer\": convnetplus_optimizer,\n",
    "            \"scheduler\": convnetplus_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": tinyvgg_model,\n",
    "            \"name\": \"TinyVGG\",\n",
    "            \"optimizer\": tinyvgg_optimizer,\n",
    "            \"scheduler\": tinyvgg_scheduler\n",
    "        },\n",
    "        {\n",
    "            \"model\": efficientnetv2_model,\n",
    "            \"name\": \"EfficientNetV2\",\n",
    "            \"optimizer\": efficientnetv2_optimizer,\n",
    "            \"scheduler\": efficientnetv2_scheduler\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for m in models:\n",
    "        print(f\"\\n{'='*50}\\nStarting Training for {m['name']}\\n{'='*50}\")\n",
    "        wandb.run.name = f\"{m['name']}_Training\"\n",
    "        wandb.run.save()\n",
    "        train_model(\n",
    "            model=m[\"model\"],\n",
    "            model_name=m[\"name\"],\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=m[\"optimizer\"],\n",
    "            scheduler=m[\"scheduler\"],\n",
    "            device=device,\n",
    "            output_dirs=output_dirs,\n",
    "            num_epochs=NUM_EPOCHS\n",
    "        )\n",
    "\n",
    "    # Finalize W&B run\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
