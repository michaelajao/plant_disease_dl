{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e079c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vit_model.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='vit_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4     # Lower learning rate for training from scratch\n",
    "NUM_EPOCHS = 50          # Increased epochs for better training\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"ViT\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms with Class-Specific Augmentations\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Vision Transformer (ViT) Architecture\n",
    "# ================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Using a Conv2d layer to perform patch extraction and embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Patch embeddings of shape [batch_size, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
    "        x = x.flatten(2)  # Shape: [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_patches (int): Number of patches in the input.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize the positional embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positionally encoded tensor of shape [batch_size, num_patches + 1, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # [CLS] token: a learnable embedding prepended to the patch embeddings\n",
    "        cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).to(x.device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # Concatenate [CLS] token with patch embeddings\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, embed_dim = x.size()\n",
    "        \n",
    "        # Linear projection and split into Q, K, V\n",
    "        qkv = self.qkv(x)  # Shape: [batch_size, num_tokens, 3 * embed_dim]\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, num_tokens, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = attn_scores.softmax(dim=-1)  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_probs @ v  # Shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)  # Shape: [batch_size, num_tokens, num_heads, head_dim]\n",
    "        attn_output = attn_output.flatten(2)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        out = self.proj(attn_output)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        # MHSA block with residual connection\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_ratio=4.0, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            depth (int): Number of transformer encoder blocks.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(embed_dim, num_patches, dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        x = self.pos_embed(x)    # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.norm(x)         # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # [CLS] token is the first token\n",
    "        cls_token = x[:, 0]      # Shape: [batch_size, embed_dim]\n",
    "        logits = self.cls_head(cls_token)  # Shape: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "# Model Initialization\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Initialize Vision Transformer model from scratch\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=HEIGHT,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=output_size,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the configured device\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    vit_model = nn.DataParallel(vit_model)\n",
    "\n",
    "# ================================================================\n",
    "# Loss Function and Optimizer\n",
    "# ================================================================\n",
    "\n",
    "# Remove class weighting from loss function since we're using WeightedRandomSampler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with a lower learning rate for training from scratch\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler, epoch, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = dict(zip(unique, counts))\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, criterion, device, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize W&B\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"ViT_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"model\": \"VisionTransformer\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"num_classes\": output_size,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "# Initialize EarlyStopping and ModelCheckpoint\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOPPING_PATIENCE, \n",
    "    verbose=True, \n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\")\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\"), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "# ================================================================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Callbacks and Mixed Precision\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Time tracking\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=vit_model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model=vit_model,\n",
    "        dataloader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({f\"{vit_model.__class__.__name__}/learning_rate\": current_lr})\n",
    "    \n",
    "    # Model checkpoint based on validation loss\n",
    "    model_checkpoint(val_loss, vit_model)\n",
    "    \n",
    "    # Early Stopping based on validation loss\n",
    "    early_stopping(val_loss, vit_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "# Log total training time to W&B\n",
    "wandb.log({\"total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "# ================================================================\n",
    "# Visualization and Saving Artifacts\n",
    "# ================================================================\n",
    "\n",
    "# Plot training metrics\n",
    "plot_save_path = os.path.join(output_dirs[1], \"ViT_training_validation_metrics.png\")\n",
    "plot_training_metrics(\n",
    "    train_losses, \n",
    "    train_accuracies, \n",
    "    val_losses, \n",
    "    val_accuracies, \n",
    "    model_name=\"ViT\",\n",
    "    save_path=plot_save_path\n",
    ")\n",
    "\n",
    "# Perform post-training evaluation on the validation set\n",
    "evaluate_model_post_training(\n",
    "    model=vit_model, \n",
    "    dataloader=valid_loader, \n",
    "    device=device, \n",
    "    idx_to_disease=idx_to_disease, \n",
    "    model_name=\"ViT\", \n",
    "    save_dir=output_dirs[1]\n",
    ")\n",
    "\n",
    "# Save W&B artifacts\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79853302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vit_model.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='vit_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4     # Lower learning rate for training from scratch\n",
    "NUM_EPOCHS = 50          # Increased epochs for better training\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"ViT\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms with Class-Specific Augmentations\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Vision Transformer (ViT) Architecture\n",
    "# ================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Using a Conv2d layer to perform patch extraction and embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Patch embeddings of shape [batch_size, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
    "        x = x.flatten(2)  # Shape: [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_patches (int): Number of patches in the input.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize the positional embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positionally encoded tensor of shape [batch_size, num_patches + 1, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # [CLS] token: a learnable embedding prepended to the patch embeddings\n",
    "        cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).to(x.device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # Concatenate [CLS] token with patch embeddings\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, embed_dim = x.size()\n",
    "        \n",
    "        # Linear projection and split into Q, K, V\n",
    "        qkv = self.qkv(x)  # Shape: [batch_size, num_tokens, 3 * embed_dim]\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, num_tokens, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = attn_scores.softmax(dim=-1)  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_probs @ v  # Shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)  # Shape: [batch_size, num_tokens, num_heads, head_dim]\n",
    "        attn_output = attn_output.flatten(2)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        out = self.proj(attn_output)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        # MHSA block with residual connection\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_ratio=4.0, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            depth (int): Number of transformer encoder blocks.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(embed_dim, num_patches, dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        x = self.pos_embed(x)    # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.norm(x)         # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # [CLS] token is the first token\n",
    "        cls_token = x[:, 0]      # Shape: [batch_size, embed_dim]\n",
    "        logits = self.cls_head(cls_token)  # Shape: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "# Model Initialization\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Initialize Vision Transformer model from scratch\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=HEIGHT,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=output_size,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the configured device\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    vit_model = nn.DataParallel(vit_model)\n",
    "\n",
    "# ================================================================\n",
    "# Loss Function and Optimizer\n",
    "# ================================================================\n",
    "\n",
    "# Remove class weighting from loss function since we're using WeightedRandomSampler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with a lower learning rate for training from scratch\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model_name = \"ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler, epoch, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = dict(zip(unique, counts))\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, criterion, device, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize W&B\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"ViT_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"model\": \"VisionTransformer\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"num_classes\": output_size,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "# Initialize EarlyStopping and ModelCheckpoint\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOPPING_PATIENCE, \n",
    "    verbose=True, \n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\")\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\"), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "# ================================================================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Callbacks and Mixed Precision\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Time tracking\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=vit_model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model=vit_model,\n",
    "        dataloader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({f\"{vit_model.__class__.__name__}/learning_rate\": current_lr})\n",
    "    \n",
    "    # Model checkpoint based on validation loss\n",
    "    model_checkpoint(val_loss, vit_model)\n",
    "    \n",
    "    # Early Stopping based on validation loss\n",
    "    early_stopping(val_loss, vit_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "# Log total training time to W&B\n",
    "wandb.log({\"total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "# ================================================================\n",
    "# Visualization and Saving Artifacts\n",
    "# ================================================================\n",
    "\n",
    "# Plot training metrics\n",
    "plot_save_path = os.path.join(output_dirs[1], \"ViT_training_validation_metrics.png\")\n",
    "plot_training_metrics(\n",
    "    train_losses, \n",
    "    train_accuracies, \n",
    "    val_losses, \n",
    "    val_accuracies, \n",
    "    model_name=\"ViT\",\n",
    "    save_path=plot_save_path\n",
    ")\n",
    "\n",
    "# Perform post-training evaluation on the validation set\n",
    "evaluate_model_post_training(\n",
    "    model=vit_model, \n",
    "    dataloader=valid_loader, \n",
    "    device=device, \n",
    "    idx_to_disease=idx_to_disease, \n",
    "    model_name=\"ViT\", \n",
    "    save_dir=output_dirs[1]\n",
    ")\n",
    "\n",
    "# Save W&B artifacts\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13842968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vit_model.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='vit_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4     # Lower learning rate for training from scratch\n",
    "NUM_EPOCHS = 50          # Increased epochs for better training\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_ViT\"\n",
    "model_name = \"ViT\"  # Define model name globally\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"ViT\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms with Class-Specific Augmentations\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Vision Transformer (ViT) Architecture\n",
    "# ================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Using a Conv2d layer to perform patch extraction and embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Patch embeddings of shape [batch_size, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
    "        x = x.flatten(2)  # Shape: [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_patches (int): Number of patches in the input.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize the positional embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positionally encoded tensor of shape [batch_size, num_patches + 1, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # [CLS] token: a learnable embedding prepended to the patch embeddings\n",
    "        cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).to(x.device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # Concatenate [CLS] token with patch embeddings\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, embed_dim = x.size()\n",
    "        \n",
    "        # Linear projection and split into Q, K, V\n",
    "        qkv = self.qkv(x)  # Shape: [batch_size, num_tokens, 3 * embed_dim]\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, num_tokens, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = attn_scores.softmax(dim=-1)  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_probs @ v  # Shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)  # Shape: [batch_size, num_tokens, num_heads, head_dim]\n",
    "        attn_output = attn_output.flatten(2)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        out = self.proj(attn_output)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        # MHSA block with residual connection\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_ratio=4.0, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            depth (int): Number of transformer encoder blocks.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(embed_dim, num_patches, dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        x = self.pos_embed(x)    # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.norm(x)         # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # [CLS] token is the first token\n",
    "        cls_token = x[:, 0]      # Shape: [batch_size, embed_dim]\n",
    "        logits = self.cls_head(cls_token)  # Shape: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "# Model Initialization\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Initialize Vision Transformer model from scratch\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=HEIGHT,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=output_size,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the configured device\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    vit_model = nn.DataParallel(vit_model)\n",
    "\n",
    "# ================================================================\n",
    "# Loss Function and Optimizer\n",
    "# ================================================================\n",
    "\n",
    "# Remove class weighting from loss function since we're using WeightedRandomSampler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with a lower learning rate for training from scratch\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model_name = \"ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler, epoch, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = dict(zip(unique, counts))\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, criterion, device, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize W&B\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"ViT_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"model\": \"VisionTransformer\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"num_classes\": output_size,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "# Initialize EarlyStopping and ModelCheckpoint\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOPPING_PATIENCE, \n",
    "    verbose=True, \n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\")\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\"), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "# ================================================================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Callbacks and Mixed Precision\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Time tracking\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=vit_model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model=vit_model,\n",
    "        dataloader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({f\"{vit_model.__class__.__name__}/learning_rate\": current_lr})\n",
    "    \n",
    "    # Model checkpoint based on validation loss\n",
    "    model_checkpoint(val_loss, vit_model)\n",
    "    \n",
    "    # Early Stopping based on validation loss\n",
    "    early_stopping(val_loss, vit_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "# Log total training time to W&B\n",
    "wandb.log({\"total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "# ================================================================\n",
    "# Visualization and Saving Artifacts\n",
    "# ================================================================\n",
    "\n",
    "# Plot training metrics\n",
    "plot_save_path = os.path.join(output_dirs[1], \"ViT_training_validation_metrics.png\")\n",
    "plot_training_metrics(\n",
    "    train_losses, \n",
    "    train_accuracies, \n",
    "    val_losses, \n",
    "    val_accuracies, \n",
    "    model_name=\"ViT\",\n",
    "    save_path=plot_save_path\n",
    ")\n",
    "\n",
    "# Perform post-training evaluation on the validation set\n",
    "evaluate_model_post_training(\n",
    "    model=vit_model, \n",
    "    dataloader=valid_loader, \n",
    "    device=device, \n",
    "    idx_to_disease=idx_to_disease, \n",
    "    model_name=\"ViT\", \n",
    "    save_dir=output_dirs[1]\n",
    ")\n",
    "\n",
    "# Save W&B artifacts\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vit_model.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='vit_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4     # Lower learning rate for training from scratch\n",
    "NUM_EPOCHS = 50          # Increased epochs for better training\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_ViT\"\n",
    "model_name = \"ViT\"  # Define model name globally\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"ViT\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms with Class-Specific Augmentations\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Vision Transformer (ViT) Architecture\n",
    "# ================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Using a Conv2d layer to perform patch extraction and embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Patch embeddings of shape [batch_size, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
    "        x = x.flatten(2)  # Shape: [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_patches (int): Number of patches in the input.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize the positional embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positionally encoded tensor of shape [batch_size, num_patches + 1, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # [CLS] token: a learnable embedding prepended to the patch embeddings\n",
    "        cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).to(x.device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # Concatenate [CLS] token with patch embeddings\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, embed_dim = x.size()\n",
    "        \n",
    "        # Linear projection and split into Q, K, V\n",
    "        qkv = self.qkv(x)  # Shape: [batch_size, num_tokens, 3 * embed_dim]\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, num_tokens, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = attn_scores.softmax(dim=-1)  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_probs @ v  # Shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)  # Shape: [batch_size, num_tokens, num_heads, head_dim]\n",
    "        attn_output = attn_output.flatten(2)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        out = self.proj(attn_output)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        # MHSA block with residual connection\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_ratio=4.0, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            depth (int): Number of transformer encoder blocks.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(embed_dim, num_patches, dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        x = self.pos_embed(x)    # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.norm(x)         # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # [CLS] token is the first token\n",
    "        cls_token = x[:, 0]      # Shape: [batch_size, embed_dim]\n",
    "        logits = self.cls_head(cls_token)  # Shape: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "# Model Initialization\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Initialize Vision Transformer model from scratch\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=HEIGHT,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=output_size,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the configured device\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    vit_model = nn.DataParallel(vit_model)\n",
    "\n",
    "# ================================================================\n",
    "# Loss Function and Optimizer\n",
    "# ================================================================\n",
    "\n",
    "# Remove class weighting from loss function since we're using WeightedRandomSampler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with a lower learning rate for training from scratch\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model_name = \"ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device, scaler, epoch, model_name, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f\"{model_name} - Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            # Convert keys to strings to avoid TypeError\n",
    "            class_distribution = {str(int(k)): int(v) for k, v in zip(unique, counts)}\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"{model_name} - Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize W&B\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"ViT_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"model\": \"VisionTransformer\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"num_classes\": output_size,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "# Initialize EarlyStopping and ModelCheckpoint\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOPPING_PATIENCE, \n",
    "    verbose=True, \n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\")\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\"), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "# ================================================================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Callbacks and Mixed Precision\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Time tracking\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=vit_model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model=vit_model,\n",
    "        dataloader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({f\"{vit_model.__class__.__name__}/learning_rate\": current_lr})\n",
    "    \n",
    "    # Model checkpoint based on validation loss\n",
    "    model_checkpoint(val_loss, vit_model)\n",
    "    \n",
    "    # Early Stopping based on validation loss\n",
    "    early_stopping(val_loss, vit_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "# Log total training time to W&B\n",
    "wandb.log({\"total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "# ================================================================\n",
    "# Visualization and Saving Artifacts\n",
    "# ================================================================\n",
    "\n",
    "# Plot training metrics\n",
    "plot_save_path = os.path.join(output_dirs[1], \"ViT_training_validation_metrics.png\")\n",
    "plot_training_metrics(\n",
    "    train_losses, \n",
    "    train_accuracies, \n",
    "    val_losses, \n",
    "    val_accuracies, \n",
    "    model_name=\"ViT\",\n",
    "    save_path=plot_save_path\n",
    ")\n",
    "\n",
    "# Perform post-training evaluation on the validation set\n",
    "evaluate_model_post_training(\n",
    "    model=vit_model, \n",
    "    dataloader=valid_loader, \n",
    "    device=device, \n",
    "    idx_to_disease=idx_to_disease, \n",
    "    model_name=\"ViT\", \n",
    "    save_dir=output_dirs[1]\n",
    ")\n",
    "\n",
    "# Save W&B artifacts\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e610fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qt13n3tw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ViT_Training</strong> at: <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT/runs/qt13n3tw' target=\"_blank\">https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT/runs/qt13n3tw</a><br/> View project at: <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT' target=\"_blank\">https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241021_102209-qt13n3tw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qt13n3tw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olarinoyem/Research/plant_disease_dl/src/models/wandb/run-20241021_102338-7l4euo75</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT/runs/7l4euo75' target=\"_blank\">ViT_Training</a></strong> to <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT' target=\"_blank\">https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT/runs/7l4euo75' target=\"_blank\">https://wandb.ai/michaelajao-ml/Plant_Leaf_Disease_ViT/runs/7l4euo75</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_vit_model.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='vit_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4     # Lower learning rate for training from scratch\n",
    "NUM_EPOCHS = 50          # Increased epochs for better training\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_ViT\"\n",
    "model_name = \"ViT\"  # Define model name globally\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"ViT\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms with Class-Specific Augmentations\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Vision Transformer (ViT) Architecture\n",
    "# ================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Using a Conv2d layer to perform patch extraction and embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Patch embeddings of shape [batch_size, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
    "        x = x.flatten(2)  # Shape: [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_patches (int): Number of patches in the input.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize the positional embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positionally encoded tensor of shape [batch_size, num_patches + 1, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # [CLS] token: a learnable embedding prepended to the patch embeddings\n",
    "        cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).to(x.device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # Concatenate [CLS] token with patch embeddings\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, embed_dim = x.size()\n",
    "        \n",
    "        # Linear projection and split into Q, K, V\n",
    "        qkv = self.qkv(x)  # Shape: [batch_size, num_tokens, 3 * embed_dim]\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, num_tokens, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = attn_scores.softmax(dim=-1)  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_probs @ v  # Shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)  # Shape: [batch_size, num_tokens, num_heads, head_dim]\n",
    "        attn_output = attn_output.flatten(2)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        out = self.proj(attn_output)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        # MHSA block with residual connection\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_ratio=4.0, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            depth (int): Number of transformer encoder blocks.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(embed_dim, num_patches, dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        x = self.pos_embed(x)    # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.norm(x)         # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # [CLS] token is the first token\n",
    "        cls_token = x[:, 0]      # Shape: [batch_size, embed_dim]\n",
    "        logits = self.cls_head(cls_token)  # Shape: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "# Model Initialization\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Initialize Vision Transformer model from scratch\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=HEIGHT,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=output_size,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the configured device\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    vit_model = nn.DataParallel(vit_model)\n",
    "\n",
    "# ================================================================\n",
    "# Loss Function and Optimizer\n",
    "# ================================================================\n",
    "\n",
    "# Remove class weighting from loss function since we're using WeightedRandomSampler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with a lower learning rate for training from scratch\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model_name = \"ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device, scaler, epoch, model_name, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        model_name (str): Name of the model for logging.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f\"{model_name} - Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            # Convert keys to strings to avoid TypeError\n",
    "            class_distribution = {str(int(k)): int(v) for k, v in zip(unique, counts)}\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"{model_name} - Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize W&B\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"ViT_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"model\": \"VisionTransformer\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"num_classes\": output_size,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "# Initialize EarlyStopping and ModelCheckpoint\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOPPING_PATIENCE, \n",
    "    verbose=True, \n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\")\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\"), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "# ================================================================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Callbacks and Mixed Precision\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Time tracking\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=vit_model,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model=vit_model,\n",
    "        dataloader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({f\"{vit_model.__class__.__name__}/learning_rate\": current_lr})\n",
    "    \n",
    "    # Model checkpoint based on validation loss\n",
    "    model_checkpoint(val_loss, vit_model)\n",
    "    \n",
    "    # Early Stopping based on validation loss\n",
    "    early_stopping(val_loss, vit_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "# Log total training time to W&B\n",
    "wandb.log({\"total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "# ================================================================\n",
    "# Visualization and Saving Artifacts\n",
    "# ================================================================\n",
    "\n",
    "# Plot training metrics\n",
    "plot_save_path = os.path.join(output_dirs[1], \"ViT_training_validation_metrics.png\")\n",
    "plot_training_metrics(\n",
    "    train_losses, \n",
    "    train_accuracies, \n",
    "    val_losses, \n",
    "    val_accuracies, \n",
    "    model_name=\"ViT\",\n",
    "    save_path=plot_save_path\n",
    ")\n",
    "\n",
    "# Perform post-training evaluation on the validation set\n",
    "evaluate_model_post_training(\n",
    "    model=vit_model, \n",
    "    dataloader=valid_loader, \n",
    "    device=device, \n",
    "    idx_to_disease=idx_to_disease, \n",
    "    model_name=\"ViT\", \n",
    "    save_dir=output_dirs[1]\n",
    ")\n",
    "\n",
    "# Save W&B artifacts\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15e4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vit_model.py\n",
    "\n",
    "# ================================================================\n",
    "# Import Necessary Libraries\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# For Weights & Biases integration\n",
    "import wandb\n",
    "\n",
    "# For model definitions\n",
    "import timm\n",
    "\n",
    "# ================================================================\n",
    "# Helper Functions and Settings\n",
    "# ================================================================\n",
    "# Assuming helper_functions.py exists and contains set_seeds\n",
    "# Adjust the path as necessary\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from helper_functions import set_seeds  # Adjust import based on your project structure\n",
    "\n",
    "# ================================================================\n",
    "# Setup Logging\n",
    "# ================================================================\n",
    "logging.basicConfig(\n",
    "    filename='vit_training_errors.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Configuration and Settings\n",
    "# ================================================================\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64          # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4     # Lower learning rate for training from scratch\n",
    "NUM_EPOCHS = 50          # Increased epochs for better training\n",
    "HEIGHT, WIDTH = 224, 224 # Image dimensions\n",
    "\n",
    "# Early Stopping Parameters\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for early stopping\n",
    "\n",
    "# W&B Project Name\n",
    "WANDB_PROJECT_NAME = \"Plant_Leaf_Disease_ViT\"\n",
    "model_name = \"ViT\"  # Define model name globally\n",
    "\n",
    "# ================================================================\n",
    "# Device Configuration\n",
    "# ================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 1:\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================================\n",
    "# Directory Setup\n",
    "# ================================================================\n",
    "\n",
    "# Define project root (assuming this script is in the project root)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\"))\n",
    "\n",
    "# Define directories for data and models\n",
    "data_path = os.path.join(\n",
    "    project_root,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"plant_leaf_disease_dataset\",\n",
    "    \"single_task_disease\",\n",
    ")\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "valid_dir = os.path.join(data_path, \"valid\")\n",
    "\n",
    "# Define output directories for results, figures, and models\n",
    "output_dirs = [\n",
    "    os.path.join(project_root, \"reports\", \"results\"),\n",
    "    os.path.join(project_root, \"reports\", \"figures\"),\n",
    "    os.path.join(project_root, \"models\", \"ViT\"),\n",
    "]\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for directory in output_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to list directory contents\n",
    "def list_directory_contents(directory, num_items=10):\n",
    "    if os.path.exists(directory):\n",
    "        contents = os.listdir(directory)\n",
    "        print(\n",
    "            f\"Contents of {directory} ({len(contents)} items): {contents[:num_items]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "# Verify directories and list contents\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(valid_dir)}\")\n",
    "list_directory_contents(train_dir, num_items=10)\n",
    "list_directory_contents(valid_dir, num_items=10)\n",
    "\n",
    "# ================================================================\n",
    "# Load Label Mappings\n",
    "# ================================================================\n",
    "\n",
    "# Path to label mapping JSON\n",
    "labels_mapping_path = os.path.join(data_path, \"labels_mapping_single_task_disease.json\")\n",
    "\n",
    "# Load the label mapping\n",
    "if os.path.exists(labels_mapping_path):\n",
    "    with open(labels_mapping_path, \"r\") as f:\n",
    "        labels_mapping = json.load(f)\n",
    "\n",
    "    disease_to_idx = labels_mapping.get(\"disease_to_idx\", {})\n",
    "    if not disease_to_idx:\n",
    "        print(\"Error: 'disease_to_idx' mapping not found in the JSON file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    idx_to_disease = {v: k for k, v in disease_to_idx.items()}\n",
    "    print(f\"Disease to Index Mapping: {disease_to_idx}\")\n",
    "    print(f\"Index to Disease Mapping: {idx_to_disease}\")\n",
    "else:\n",
    "    print(f\"Warning: Label mapping file not found at {labels_mapping_path}. Exiting.\")\n",
    "    sys.exit(1)  # Exit, as proper label mapping is essential\n",
    "\n",
    "# ================================================================\n",
    "# Define Minority Classes\n",
    "# ================================================================\n",
    "\n",
    "# Define minority classes based on training label counts\n",
    "# You can adjust the threshold as needed\n",
    "minority_threshold = 1000  # Classes with fewer than 1000 samples are considered minority\n",
    "\n",
    "# Path to training split CSV\n",
    "train_split_csv = os.path.join(data_path, \"train_split.csv\")\n",
    "if os.path.exists(train_split_csv):\n",
    "    train_df = pd.read_csv(train_split_csv)\n",
    "    train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "else:\n",
    "    print(f\"Error: Training split CSV not found at {train_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "minority_classes = train_label_counts[train_label_counts < minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nIdentified Minority Classes (count < {minority_threshold}):\")\n",
    "for cls in minority_classes:\n",
    "    print(f\"Class {cls} ({idx_to_disease.get(cls, 'Unknown')}) with {train_label_counts[cls]} samples\")\n",
    "\n",
    "# ================================================================\n",
    "# Custom Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform_major=None, transform_minority=None,\n",
    "                 minority_classes=None, image_col='image', label_col='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            images_dir (str): Directory with all the images.\n",
    "            transform_major (callable, optional): Transformations for majority classes.\n",
    "            transform_minority (callable, optional): Transformations for minority classes.\n",
    "            minority_classes (list, optional): List of minority class indices.\n",
    "            image_col (str): Column name for image filenames in the CSV.\n",
    "            label_col (str): Column name for labels in the CSV.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform_major = transform_major\n",
    "        self.transform_minority = transform_minority\n",
    "        self.minority_classes = minority_classes if minority_classes else []\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        # Verify required columns\n",
    "        required_columns = [image_col, label_col]\n",
    "        for col in required_columns:\n",
    "            if col not in self.annotations.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in CSV file.\")\n",
    "\n",
    "        # Ensure labels are integers\n",
    "        if not pd.api.types.is_integer_dtype(self.annotations[self.label_col]):\n",
    "            try:\n",
    "                self.annotations[self.label_col] = self.annotations[self.label_col].astype(int)\n",
    "                print(f\"Converted labels in {csv_file} to integers.\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: Labels in {csv_file} cannot be converted to integers.\")\n",
    "                self.annotations[self.label_col] = -1  # Assign invalid label\n",
    "\n",
    "        # Debug: Print unique labels after conversion\n",
    "        unique_labels = self.annotations[self.label_col].unique()\n",
    "        print(f\"Unique labels after conversion in {csv_file}: {unique_labels}\")\n",
    "\n",
    "        # Check labels are within [0, num_classes - 1]\n",
    "        num_classes = len(disease_to_idx)\n",
    "        valid_labels = self.annotations[self.label_col].between(0, num_classes - 1)\n",
    "        invalid_count = len(self.annotations) - valid_labels.sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"Found {invalid_count} samples with invalid labels in {csv_file}. These will be skipped.\")\n",
    "            self.annotations = self.annotations[valid_labels].reset_index(drop=True)\n",
    "\n",
    "        # Final count\n",
    "        print(f\"Number of samples after filtering in {csv_file}: {len(self.annotations)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and label\n",
    "        img_name_full = self.annotations.iloc[idx][self.image_col]\n",
    "        label_idx = self.annotations.iloc[idx][self.label_col]\n",
    "\n",
    "        # Extract only the basename to avoid path duplication\n",
    "        img_name = os.path.basename(img_name_full)\n",
    "\n",
    "        # Full path to the image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            image = Image.new(\"RGB\", (HEIGHT, WIDTH), (0, 0, 0))\n",
    "\n",
    "        # Apply class-specific transformations\n",
    "        if label_idx in self.minority_classes and self.transform_minority:\n",
    "            image = self.transform_minority(image)\n",
    "        elif self.transform_major:\n",
    "            image = self.transform_major(image)\n",
    "\n",
    "        return image, label_idx\n",
    "\n",
    "# ================================================================\n",
    "# Data Transforms with Class-Specific Augmentations\n",
    "# ================================================================\n",
    "\n",
    "# Define transforms for majority classes\n",
    "transform_major = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Define transforms for minority classes with additional augmentations\n",
    "transform_minority = transforms.Compose([\n",
    "    transforms.Resize((HEIGHT, WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Additional flip\n",
    "    transforms.RandomRotation(30),    # More rotation\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Color jitter\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # Mean for ImageNet\n",
    "        std=[0.229, 0.224, 0.225]     # Std for ImageNet\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ================================================================\n",
    "# Initialize Datasets and DataLoaders (Using WeightedRandomSampler)\n",
    "# ================================================================\n",
    "\n",
    "# Path to validation split CSV\n",
    "valid_split_csv = os.path.join(data_path, \"valid_split.csv\")\n",
    "if os.path.exists(valid_split_csv):\n",
    "    valid_df = pd.read_csv(valid_split_csv)\n",
    "    valid_dataset = PlantDiseaseDataset(\n",
    "        csv_file=valid_split_csv,\n",
    "        images_dir=valid_dir,\n",
    "        transform_major=transform_major,  # Validation should not have augmentation\n",
    "        transform_minority=None,          # No augmentation for validation\n",
    "        minority_classes=[],              # No augmentation needed\n",
    "        image_col='image',\n",
    "        label_col='label'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Error: Validation split CSV not found at {valid_split_csv}. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize training dataset\n",
    "train_dataset = PlantDiseaseDataset(\n",
    "    csv_file=train_split_csv,\n",
    "    images_dir=train_dir,\n",
    "    transform_major=transform_major,\n",
    "    transform_minority=transform_minority,\n",
    "    minority_classes=minority_classes,\n",
    "    image_col='image',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "# Create WeightedRandomSampler for the training DataLoader\n",
    "# Compute class counts and weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1. / class_counts\n",
    "samples_weight = class_weights[train_df['label'].values]\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weight,\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nNumber of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\")\n",
    "print(f\"Number of classes: {len(disease_to_idx)}\")\n",
    "print(f\"Classes: {list(disease_to_idx.keys())}\")\n",
    "\n",
    "# Test fetching a single sample\n",
    "if len(train_dataset) > 0:\n",
    "    sample_image, sample_label = train_dataset[0]\n",
    "    print(f\"\\nSample Image Shape: {sample_image.shape}\")\n",
    "    print(f\"Sample Label Index: {sample_label}\")\n",
    "    print(f\"Sample Label Name: {idx_to_disease.get(sample_label, 'Unknown')}\")\n",
    "else:\n",
    "    print(\"\\nTraining dataset is empty. Please check your dataset and label mappings.\")\n",
    "\n",
    "# ================================================================\n",
    "# Vision Transformer (ViT) Architecture\n",
    "# ================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Using a Conv2d layer to perform patch extraction and embedding\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Patch embeddings of shape [batch_size, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # Shape: [batch_size, embed_dim, num_patches**0.5, num_patches**0.5]\n",
    "        x = x.flatten(2)  # Shape: [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_patches (int): Number of patches in the input.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Initialize the positional embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Positionally encoded tensor of shape [batch_size, num_patches + 1, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # [CLS] token: a learnable embedding prepended to the patch embeddings\n",
    "        cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)).to(x.device)\n",
    "        cls_token = cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # Concatenate [CLS] token with patch embeddings\n",
    "        x = torch.cat((cls_token, x), dim=1)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, embed_dim = x.size()\n",
    "        \n",
    "        # Linear projection and split into Q, K, V\n",
    "        qkv = self.qkv(x)  # Shape: [batch_size, num_tokens, 3 * embed_dim]\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, num_tokens, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = attn_scores.softmax(dim=-1)  # Shape: [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_probs @ v  # Shape: [batch_size, num_heads, num_tokens, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)  # Shape: [batch_size, num_tokens, num_heads, head_dim]\n",
    "        attn_output = attn_output.flatten(2)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        out = self.proj(attn_output)  # Shape: [batch_size, num_tokens, embed_dim]\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_tokens, embed_dim]\n",
    "        \"\"\"\n",
    "        # MHSA block with residual connection\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12, \n",
    "        num_heads=12, \n",
    "        mlp_ratio=4.0, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Size of the input image (assumed square).\n",
    "            patch_size (int): Size of each patch (assumed square).\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            num_classes (int): Number of output classes.\n",
    "            embed_dim (int): Dimension of the embedding space.\n",
    "            depth (int): Number of transformer encoder blocks.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            mlp_ratio (float): Ratio of the hidden dimension in FFN to embed_dim.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(embed_dim, num_patches, dropout)\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, in_channels, img_size, img_size]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "        x = self.pos_embed(x)    # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.transformer(x)  # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        x = self.norm(x)         # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        # [CLS] token is the first token\n",
    "        cls_token = x[:, 0]      # Shape: [batch_size, embed_dim]\n",
    "        logits = self.cls_head(cls_token)  # Shape: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "# Model Initialization\n",
    "# ================================================================\n",
    "\n",
    "# Define the number of classes\n",
    "output_size = len(disease_to_idx)\n",
    "\n",
    "# Initialize Vision Transformer model from scratch\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=HEIGHT,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    num_classes=output_size,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the configured device\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if num_gpus > 1:\n",
    "    vit_model = nn.DataParallel(vit_model)\n",
    "\n",
    "# ================================================================\n",
    "# Loss Function and Optimizer\n",
    "# ================================================================\n",
    "\n",
    "# Remove class weighting from loss function since we're using WeightedRandomSampler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with a lower learning rate for training from scratch\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model_name = \"ViT\"\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0, path='best_model.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Saves the model based on validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, path='best_val_loss_model.pth', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "            verbose (bool): If True, prints messages when saving the model.\n",
    "        \"\"\"\n",
    "        self.best_loss = None\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved ({self.best_loss if self.best_loss else 'N/A'} --> {val_loss:.6f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "\n",
    "# ================================================================\n",
    "# Training and Validation Functions\n",
    "# ================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler, epoch, log_interval=10):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using mixed precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to train on.\n",
    "        scaler (GradScaler): GradScaler for mixed precision.\n",
    "        epoch (int): Current epoch number.\n",
    "        log_interval (int): How often to log batch metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # Enhanced Logging: Log every 'log_interval' batches\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            unique, counts = np.unique(labels.cpu().numpy(), return_counts=True)\n",
    "            class_distribution = {str(int(k)): int(v) for k, v in zip(unique, counts)}\n",
    "            wandb.log({\n",
    "                f\"{model_name}/train_loss\": loss.item(),\n",
    "                f\"{model_name}/batch_train_accuracy\": torch.sum(preds == labels.data).item() / inputs.size(0),\n",
    "                f\"{model_name}/batch_class_distribution\": class_distribution\n",
    "            })\n",
    "            print(f\"Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss.item():.4f} | Class Distribution: {class_distribution}\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_train_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_train_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate(model, dataloader, criterion, device, collect_metrics=True):\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): Validation data loader.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to validate on.\n",
    "        collect_metrics (bool): If True, collect labels and predictions.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (epoch_loss, epoch_accuracy, all_labels, all_preds)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            if collect_metrics:\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"{model_name}/epoch_val_loss\": epoch_loss,\n",
    "        f\"{model_name}/epoch_val_accuracy\": epoch_acc.item()\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc.item(), all_labels, all_preds\n",
    "\n",
    "# ================================================================\n",
    "# Visualization Utilities\n",
    "# ================================================================\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, val_losses, val_accuracies, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses.\n",
    "        train_accuracies (list): List of training accuracies.\n",
    "        val_losses (list): List of validation losses.\n",
    "        val_accuracies (list): List of validation accuracies.\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        save_path (str, optional): Path to save the plot. If None, the plot is shown.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        wandb.log({f\"{model_name}/training_validation_metrics\": wandb.Image(save_path)})\n",
    "        print(f\"Training metrics plot saved at {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model_post_training(model, dataloader, device, idx_to_disease, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset and print classification metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        idx_to_disease (dict): Mapping from index to disease name.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "        save_dir (str): Directory to save the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define loss function (same as training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"{model_name} - Evaluation\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Mixed precision inference\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"\\n{model_name} - Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(idx_to_disease.values()))\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.log({f\"{model_name}/classification_report\": report})\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=list(idx_to_disease.values()), \n",
    "                yticklabels=list(idx_to_disease.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    cm_save_path = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_save_path)\n",
    "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(cm_save_path)})\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved at {cm_save_path}\")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize W&B\n",
    "# ================================================================\n",
    "\n",
    "# Initialize W&B run\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=\"ViT_Training\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"model\": \"VisionTransformer\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"num_classes\": output_size,\n",
    "        \"image_size\": f\"{HEIGHT}x{WIDTH}\",\n",
    "    },\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "# Get the run id for tracking\n",
    "run_id = wandb.run.id\n",
    "print(f\"W&B Run ID: {run_id}\")\n",
    "\n",
    "# ================================================================\n",
    "# Callbacks for Training Monitoring\n",
    "# ================================================================\n",
    "\n",
    "# Initialize EarlyStopping and ModelCheckpoint\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOPPING_PATIENCE, \n",
    "    verbose=True, \n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\")\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    path=os.path.join(output_dirs[2], \"best_val_loss_model.pth\"), \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Initialize GradScaler for Mixed Precision\n",
    "# ================================================================\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Callbacks and Mixed Precision\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Time tracking\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model=vit_model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model=vit_model,\n",
    "        dataloader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "    \n",
    "    # Append metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({f\"{vit_model.__class__.__name__}/learning_rate\": current_lr})\n",
    "    \n",
    "    # Model checkpoint based on validation loss\n",
    "    model_checkpoint(val_loss, vit_model)\n",
    "    \n",
    "    # Early Stopping based on validation loss\n",
    "    early_stopping(val_loss, vit_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"\\nTotal Training Time: {total_duration/60:.2f} minutes\")\n",
    "\n",
    "# Log total training time to W&B\n",
    "wandb.log({\"total_training_time_minutes\": total_duration/60})\n",
    "\n",
    "# ================================================================\n",
    "# Visualization and Saving Artifacts\n",
    "# ================================================================\n",
    "\n",
    "# Plot training metrics\n",
    "plot_save_path = os.path.join(output_dirs[1], \"ViT_training_validation_metrics.png\")\n",
    "plot_training_metrics(\n",
    "    train_losses, \n",
    "    train_accuracies, \n",
    "    val_losses, \n",
    "    val_accuracies, \n",
    "    model_name=\"ViT\",\n",
    "    save_path=plot_save_path\n",
    ")\n",
    "\n",
    "# Perform post-training evaluation on the validation set\n",
    "evaluate_model_post_training(\n",
    "    model=vit_model, \n",
    "    dataloader=valid_loader, \n",
    "    device=device, \n",
    "    idx_to_disease=idx_to_disease, \n",
    "    model_name=\"ViT\", \n",
    "    save_dir=output_dirs[1]\n",
    ")\n",
    "\n",
    "# Save W&B artifacts\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
